#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®ç†å°‚é–€ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚»ãƒ³ã‚¿ãƒ¼ API
HTMLãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¨é€£æºã™ã‚‹ãŸã‚ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰API
"""

import os
import re
import glob
import json
from flask import Flask, request, jsonify, render_template_string
from flask_cors import CORS
import logging

# RAGã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from enhanced_rag_system import create_enhanced_rag_system, enhanced_rag_retrieve, format_blog_links
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    print("Warning: RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å˜ç´”æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™ã€‚")

# SERPæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from serp_search_system import get_serp_search_system, search_with_serp
    SERP_AVAILABLE = True
except ImportError:
    SERP_AVAILABLE = False
    print("Warning: SERPæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚SERPæ¤œç´¢æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚")

# Notion APIã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from notion_client import Client
    NOTION_AVAILABLE = True
except ImportError:
    NOTION_AVAILABLE = False
    print("Warning: Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚Notionæ¤œç´¢æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)  # CORSã‚’æœ‰åŠ¹ã«ã—ã¦HTMLã‹ã‚‰APIã‚’å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«ã™ã‚‹

# RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
rag_db = None
if RAG_AVAILABLE:
    try:
        rag_db = create_enhanced_rag_system()
        logger.info("RAGã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.warning(f"RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—: {str(e)}")
        rag_db = None

# SERPæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
serp_system = None
if SERP_AVAILABLE:
    try:
        serp_system = get_serp_search_system()
        logger.info("SERPæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.warning(f"SERPæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—: {str(e)}")
        serp_system = None

# Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
notion_client = None
if NOTION_AVAILABLE:
    try:
        notion_api_key = os.getenv("NOTION_API_KEY") or os.getenv("NOTION_TOKEN")
        if notion_api_key:
            notion_client = Client(auth=notion_api_key)
            logger.info("Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
        else:
            logger.warning("Notion APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    except Exception as e:
        logger.warning(f"Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—: {str(e)}")
        notion_client = None

def parse_markdown_content(content):
    """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è§£æã—ã¦æ§‹é€ åŒ–"""
    import re
    
    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜æ³•ã‚’é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    # ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ# ## ###ï¼‰ã‚’é™¤å»
    content = re.sub(r'^#{1,6}\s+', '', content, flags=re.MULTILINE)
    
    # ãƒªã‚¹ãƒˆè¨˜æ³•ï¼ˆ- * +ï¼‰ã‚’é™¤å»
    content = re.sub(r'^[\s]*[-*+]\s+', '', content, flags=re.MULTILINE)
    
    # å¤ªå­—ãƒ»æ–œä½“è¨˜æ³•ã‚’é™¤å»
    content = re.sub(r'\*\*(.*?)\*\*', r'\1', content)
    content = re.sub(r'\*(.*?)\*', r'\1', content)
    
    # ãƒªãƒ³ã‚¯è¨˜æ³•ã‚’é™¤å»ï¼ˆURLã¯ä¿æŒï¼‰
    content = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', content)
    
    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
    content = re.sub(r'```.*?```', '', content, flags=re.DOTALL)
    content = re.sub(r'`([^`]+)`', r'\1', content)
    
    return content

def analyze_query(query):
    """ã‚¯ã‚¨ãƒªã‚’è§£æã—ã¦æ¤œç´¢æˆ¦ç•¥ã‚’æ±ºå®š"""
    query_lower = query.lower()
    
    # ã‚¯ã‚¨ãƒªã®ç¨®é¡ã‚’åˆ¤å®š
    query_type = {
        'is_specific_problem': False,  # å…·ä½“çš„ãªå•é¡Œ
        'is_general_category': False,  # ä¸€èˆ¬çš„ãªã‚«ãƒ†ã‚´ãƒª
        'has_action_verb': False,     # å‹•ä½œå‹•è©ã‚’å«ã‚€
        'has_symptom': False,         # ç—‡çŠ¶ã‚’å«ã‚€
        'main_keywords': [],
        'context_keywords': [],
        'priority_score': 0
    }
    
    # å…·ä½“çš„ãªå•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³
    problem_patterns = [
        r'(ãƒ•ã‚¡ãƒ³|fan|æ›æ°—|vent).*?(å›ã‚‰ãªã„|å‹•ã‹ãªã„|å‹•ä½œã—ãªã„|æ•…éšœ|ä¸å…·åˆ)',
        r'(ãƒˆã‚¤ãƒ¬|toilet).*?(ãƒ•ã‚¡ãƒ³|fan|æ›æ°—|vent).*?(å›ã‚‰ãªã„|å‹•ã‹ãªã„|å‹•ä½œã—ãªã„)',
        r'(æ°´|water).*?(å‡ºãªã„|æµã‚Œãªã„|æ¼ã‚Œã‚‹|æ¼ã‚Œ)',
        r'(é›»æº|power).*?(å…¥ã‚‰ãªã„|åˆ‡ã‚Œã‚‹|è½ã¡ã‚‹)',
        r'(ç•°éŸ³|éŸ³|é¨’éŸ³|noise).*?(ã™ã‚‹|ç™ºç”Ÿ|é³´ã‚‹)',
        r'(é›¨æ¼ã‚Š|é›¨æ¼|æ¼æ°´|æ°´æ¼ã‚Œ|æµ¸æ°´).*?(ã™ã‚‹|ç™ºç”Ÿ|èµ·ãã‚‹|ã‚ã‚‹)',
        r'(ã‚·ãƒ¼ãƒªãƒ³ã‚°|ã‚³ãƒ¼ã‚­ãƒ³ã‚°|ãƒ‘ãƒƒã‚­ãƒ³).*?(åŠ£åŒ–|ç ´æ|å‰¥ãŒã‚Œ|å‰²ã‚Œ)',
        r'(å¤©äº•|å±‹æ ¹|ãƒ«ãƒ¼ãƒ•).*?(æ°´æ»´|æ°´|æ¿¡ã‚Œ|ã‚·ãƒŸ)',
        r'(çª“|ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦).*?(æ°´|æ¿¡ã‚Œ|æŸ“ã¿|æ¼ã‚Œ)'
    ]
    
    for pattern in problem_patterns:
        if re.search(pattern, query_lower):
            query_type['is_specific_problem'] = True
            query_type['priority_score'] += 50
            break
    
    # å‹•ä½œå‹•è©ã®æ¤œå‡º
    action_verbs = ['å›ã‚‰ãªã„', 'å‹•ã‹ãªã„', 'å‹•ä½œã—ãªã„', 'å‡ºãªã„', 'æµã‚Œãªã„', 'æ¼ã‚Œã‚‹', 'æ¼ã‚Œ', 'åˆ‡ã‚Œã‚‹', 'å…¥ã‚‰ãªã„', 'é³´ã‚‹', 'ç™ºç”Ÿ', 'ã™ã‚‹', 'èµ·ãã‚‹', 'ã‚ã‚‹']
    for verb in action_verbs:
        if verb in query_lower:
            query_type['has_action_verb'] = True
            query_type['priority_score'] += 20
            break
    
    # ç—‡çŠ¶ã®æ¤œå‡º
    symptoms = ['æ•…éšœ', 'ä¸å…·åˆ', 'ç•°éŸ³', 'é¨’éŸ³', 'æ¼ã‚Œ', 'é›¨æ¼ã‚Š', 'æ°´æ¼ã‚Œ', 'æ¼æ°´', 'æµ¸æ°´', 'åˆ‡ã‚Œ', 'è½ã¡', 'æ­¢ã¾ã‚‹', 'å‹•ã‹ãªã„', 'æ¿¡ã‚Œ', 'ã‚·ãƒŸ', 'æ°´æ»´']
    for symptom in symptoms:
        if symptom in query_lower:
            query_type['has_symptom'] = True
            query_type['priority_score'] += 15
            break
    
    # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æŠ½å‡º
    main_keywords = []
    if 'ãƒ•ã‚¡ãƒ³' in query_lower or 'fan' in query_lower:
        main_keywords.append('ãƒ•ã‚¡ãƒ³')
    if 'ãƒˆã‚¤ãƒ¬' in query_lower or 'toilet' in query_lower:
        main_keywords.append('ãƒˆã‚¤ãƒ¬')
    if 'æ›æ°—' in query_lower or 'vent' in query_lower:
        main_keywords.append('æ›æ°—')
    if 'é›¨æ¼ã‚Š' in query_lower or 'é›¨æ¼' in query_lower:
        main_keywords.append('é›¨æ¼ã‚Š')
    if 'æ°´æ¼ã‚Œ' in query_lower or 'æ¼æ°´' in query_lower:
        main_keywords.append('æ°´æ¼ã‚Œ')
    if 'ã‚·ãƒ¼ãƒªãƒ³ã‚°' in query_lower or 'ã‚³ãƒ¼ã‚­ãƒ³ã‚°' in query_lower:
        main_keywords.append('ã‚·ãƒ¼ãƒªãƒ³ã‚°')
    if 'å¤©äº•' in query_lower or 'å±‹æ ¹' in query_lower or 'ãƒ«ãƒ¼ãƒ•' in query_lower:
        main_keywords.append('å¤©äº•')
    if 'çª“' in query_lower or 'ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦' in query_lower:
        main_keywords.append('çª“')
    if 'ãƒãƒƒãƒ†ãƒªãƒ¼' in query_lower or 'battery' in query_lower:
        main_keywords.append('ãƒãƒƒãƒ†ãƒªãƒ¼')
    if 'ã‚¨ã‚¢ã‚³ãƒ³' in query_lower or 'aircon' in query_lower:
        main_keywords.append('ã‚¨ã‚¢ã‚³ãƒ³')
    if 'å†·è”µåº«' in query_lower or 'refrigerator' in query_lower:
        main_keywords.append('å†·è”µåº«')
    if 'ãƒ‰ã‚¢' in query_lower or 'door' in query_lower:
        main_keywords.append('ãƒ‰ã‚¢')
    if 'çª“' in query_lower or 'window' in query_lower:
        main_keywords.append('çª“')
    if 'é–‹é–‰' in query_lower or 'é–‹ã‘é–‰ã‚' in query_lower:
        main_keywords.append('é–‹é–‰')
    if 'ä¸å…·åˆ' in query_lower or 'æ•…éšœ' in query_lower:
        main_keywords.append('ä¸å…·åˆ')
    
    query_type['main_keywords'] = main_keywords
    
    # æ–‡è„ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æŠ½å‡º
    context_keywords = []
    if 'å›ã‚‰ãªã„' in query_lower:
        context_keywords.extend(['å‹•ä½œã—ãªã„', 'å‹•ã‹ãªã„', 'æ•…éšœ', 'ä¸å…·åˆ'])
    if 'ãƒ•ã‚¡ãƒ³' in query_lower:
        context_keywords.extend(['ãƒ¢ãƒ¼ã‚¿ãƒ¼', 'æ›æ°—', 'æ’æ°—', 'é¢¨é‡'])
    if 'ãƒˆã‚¤ãƒ¬' in query_lower:
        context_keywords.extend(['ã‚«ã‚»ãƒƒãƒˆ', 'ãƒãƒªãƒ³', 'ãƒ™ãƒ³ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼'])
    if 'é›¨æ¼ã‚Š' in query_lower or 'æ°´æ¼ã‚Œ' in query_lower:
        context_keywords.extend(['ã‚·ãƒ¼ãƒªãƒ³ã‚°', 'ã‚³ãƒ¼ã‚­ãƒ³ã‚°', 'ãƒ‘ãƒƒã‚­ãƒ³', 'é˜²æ°´', 'å¤©äº•', 'å±‹æ ¹', 'çª“', 'æ¿¡ã‚Œ', 'ã‚·ãƒŸ', 'æ°´æ»´'])
    if 'ã‚·ãƒ¼ãƒªãƒ³ã‚°' in query_lower or 'ã‚³ãƒ¼ã‚­ãƒ³ã‚°' in query_lower:
        context_keywords.extend(['ãƒ‘ãƒƒã‚­ãƒ³', 'é˜²æ°´', 'åŠ£åŒ–', 'å‰¥ãŒã‚Œ', 'å‰²ã‚Œ', 'è£œä¿®'])
    if 'å¤©äº•' in query_lower or 'å±‹æ ¹' in query_lower:
        context_keywords.extend(['ãƒ«ãƒ¼ãƒ•', 'ãƒ‘ãƒãƒ«', 'ç¶™ãç›®', 'ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆ', 'ã‚·ãƒ¼ãƒªãƒ³ã‚°', 'é˜²æ°´'])
    if 'çª“' in query_lower or 'ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦' in query_lower:
        context_keywords.extend(['ãƒ¢ãƒ¼ãƒ«', 'ãƒ‘ãƒƒã‚­ãƒ³', 'æ ', 'ã‚¬ãƒ©ã‚¹', 'é˜²æ°´', 'ã‚·ãƒ¼ãƒ«'])
    if 'ãƒãƒƒãƒ†ãƒªãƒ¼' in query_lower:
        context_keywords.extend(['å……é›»', 'é›»åœ§', 'ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼', 'é‰›ãƒãƒƒãƒ†ãƒªãƒ¼', 'ãƒªãƒã‚¦ãƒ '])
    if 'ã‚¨ã‚¢ã‚³ãƒ³' in query_lower:
        context_keywords.extend(['å†·æˆ¿', 'æš–æˆ¿', 'æ¸©åº¦', 'ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼', 'å†·åª’'])
    if 'å†·è”µåº«' in query_lower:
        context_keywords.extend(['å†·å‡', 'ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼', 'å†·å´', 'æ¸©åº¦'])
    
    query_type['context_keywords'] = context_keywords
    
    return query_type

def get_related_keywords(query):
    """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—"""
    query_lower = query.lower()
    related_keywords = []
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
    keyword_mapping = {
        "ãƒãƒƒãƒ†ãƒªãƒ¼": ["å……é›»", "é›»åœ§", "ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼", "é‰›ãƒãƒƒãƒ†ãƒªãƒ¼", "ãƒªãƒã‚¦ãƒ "],
        "ã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼": ["DC-AC", "æ­£å¼¦æ³¢", "é›»æºå¤‰æ›", "å‡ºåŠ›", "å®¹é‡"],
        "ãƒˆã‚¤ãƒ¬": ["ã‚«ã‚»ãƒƒãƒˆ", "ãƒãƒªãƒ³", "ãƒ•ãƒ©ãƒƒãƒ‘ãƒ¼", "ãƒ•ã‚¡ãƒ³", "æ›æ°—"],
        "ãƒ«ãƒ¼ãƒ•ãƒ™ãƒ³ãƒˆ": ["æ›æ°—æ‰‡", "ãƒãƒƒã‚¯ã‚¹ãƒ•ã‚¡ãƒ³", "ãƒ•ã‚¡ãƒ³", "æ›æ°—", "ãƒ™ãƒ³ãƒˆ"],
        "æ°´é“": ["ãƒãƒ³ãƒ—", "çµ¦æ°´", "æ°´", "åœ§åŠ›", "ã‚¿ãƒ³ã‚¯"],
        "å†·è”µåº«": ["å†·å‡", "ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼", "å†·å´", "æ¸©åº¦"],
        "ã‚¬ã‚¹": ["ã‚³ãƒ³ãƒ­", "ãƒ’ãƒ¼ã‚¿ãƒ¼", "FF", "ç‡ƒç„¼", "ç‚¹ç«"],
        "é›»æ°—": ["LED", "ç…§æ˜", "é›»è£…", "é…ç·š", "ãƒ’ãƒ¥ãƒ¼ã‚º"],
        "é›¨æ¼ã‚Š": ["é˜²æ°´", "ã‚·ãƒ¼ãƒ«", "ã‚·ãƒ¼ãƒªãƒ³ã‚°", "æ¼ã‚Œ", "æ°´"],
        "ç•°éŸ³": ["éŸ³", "é¨’éŸ³", "æŒ¯å‹•", "ãƒã‚¤ã‚º", "ã†ã‚‹ã•ã„"],
        "ãƒ•ã‚¡ãƒ³": ["ãƒ¢ãƒ¼ã‚¿ãƒ¼", "æ›æ°—", "æ’æ°—", "é¢¨é‡", "å›è»¢"],
        "ã‚¨ã‚¢ã‚³ãƒ³": ["å†·æˆ¿", "æš–æˆ¿", "æ¸©åº¦", "ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼", "å†·åª’"],
        "ãƒ‰ã‚¢": ["çª“", "é–‹é–‰", "ãƒ‘ãƒƒã‚­ãƒ³", "ã‚·ãƒ¼ãƒ«", "ã‚¬ã‚¿ã¤ã", "ãƒ’ãƒ³ã‚¸", "ãƒ­ãƒƒã‚¯", "å»ºä»˜ã‘", "éš™é–“é¢¨", "é–‹ã‘é–‰ã‚", "ä¸å…·åˆ", "æ•…éšœ", "å‹•ä½œä¸è‰¯", "å›ºã„", "å‹•ã‹ãªã„", "é–‰ã¾ã‚‰ãªã„", "é–‹ã‹ãªã„"],
        "çª“": ["ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦", "é–‹é–‰", "ãƒ‘ãƒƒã‚­ãƒ³", "ã‚·ãƒ¼ãƒ«", "ã‚¬ãƒ©ã‚¹", "ãƒ¬ãƒ¼ãƒ«", "ç¶²æˆ¸", "ã‚·ã‚§ãƒ¼ãƒ‰", "é–‹ã‘é–‰ã‚", "ä¸å…·åˆ", "æ•…éšœ", "å‹•ä½œä¸è‰¯", "å›ºã„", "å‹•ã‹ãªã„", "é–‰ã¾ã‚‰ãªã„", "é–‹ã‹ãªã„"],
        "é–‹é–‰": ["ãƒ‰ã‚¢", "çª“", "ãƒ’ãƒ³ã‚¸", "ãƒ¬ãƒ¼ãƒ«", "å‹•ä½œä¸è‰¯", "å›ºã„", "å‹•ã‹ãªã„", "ä¸å…·åˆ", "æ•…éšœ", "é–‹ã‘é–‰ã‚", "é–‰ã¾ã‚‰ãªã„", "é–‹ã‹ãªã„"],
        "é–‹ã‘é–‰ã‚": ["ãƒ‰ã‚¢", "çª“", "ãƒ’ãƒ³ã‚¸", "ãƒ¬ãƒ¼ãƒ«", "å‹•ä½œä¸è‰¯", "å›ºã„", "å‹•ã‹ãªã„", "ä¸å…·åˆ", "æ•…éšœ", "é–‹é–‰", "é–‰ã¾ã‚‰ãªã„", "é–‹ã‹ãªã„"],
        "ä¸å…·åˆ": ["ãƒ‰ã‚¢", "çª“", "ãƒ’ãƒ³ã‚¸", "ãƒ¬ãƒ¼ãƒ«", "å‹•ä½œä¸è‰¯", "å›ºã„", "å‹•ã‹ãªã„", "é–‹ã‘é–‰ã‚", "é–‹é–‰", "é–‰ã¾ã‚‰ãªã„", "é–‹ã‹ãªã„", "æ•…éšœ"],
        "ãƒ’ãƒ¥ãƒ¼ã‚º": ["ãƒªãƒ¬ãƒ¼", "é›»æ°—", "ã‚·ãƒ§ãƒ¼ãƒˆ", "æ–­ç·š", "é›»æº"],
        "ãƒãƒ³ãƒ—": ["æ°´", "çµ¦æ°´", "åœ§åŠ›", "æµé‡", "ãƒ¢ãƒ¼ã‚¿ãƒ¼"],
        "ã‚½ãƒ¼ãƒ©ãƒ¼": ["ãƒ‘ãƒãƒ«", "ç™ºé›»", "å¤ªé™½å…‰", "å……é›»", "ãƒãƒƒãƒ†ãƒªãƒ¼"],
        "ã‚¿ã‚¤ãƒ¤": ["ç©ºæ°—åœ§", "ãƒ‘ãƒ³ã‚¯", "æ‘©è€—", "äº¤æ›", "ãƒ›ã‚¤ãƒ¼ãƒ«"],
        "ãƒ’ãƒ¼ã‚¿ãƒ¼": ["æš–æˆ¿", "ã‚¬ã‚¹", "FF", "ç‡ƒç„¼", "æ¸©åº¦"],
        "LED": ["ç…§æ˜", "é›»çƒ", "å…‰", "é›»æ°—", "é›»è£…"],
        "å®¶å…·": ["ãƒ†ãƒ¼ãƒ–ãƒ«", "æ¤…å­", "ãƒ™ãƒƒãƒ‰", "åç´", "å›ºå®š"],
        "æ’æ°´": ["ã‚¿ãƒ³ã‚¯", "æ°´", "é…ç®¡", "è©°ã¾ã‚Š", "æ¼ã‚Œ"]
    }
    
    # ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
    for main_keyword, related_list in keyword_mapping.items():
        if main_keyword in query_lower:
            related_keywords.extend(related_list)
        
        # é€†æ–¹å‘ã®æ¤œç´¢ã‚‚è¡Œã†
        for related_word in related_list:
            if related_word in query_lower and main_keyword not in related_keywords:
                related_keywords.append(main_keyword)
    
    # é‡è¤‡ã‚’é™¤å»
    return list(set(related_keywords))

def format_search_results(results, query):
    """æ¤œç´¢çµæœã‚’æ•´ç†ã—ã¦è¡¨ç¤ºç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    formatted_results = []
    query_analysis = analyze_query(query)
    
    for i, result in enumerate(results):
        # åŸºæœ¬æƒ…å ±
        formatted_result = {
            'rank': i + 1,
            'title': result.get('title', 'ä¿®ç†ã‚¢ãƒ‰ãƒã‚¤ã‚¹'),
            'category': result.get('category', ''),
            'filename': result.get('filename', ''),
            'score': result.get('score', 0),
            'source': result.get('source', 'text_file'),
            'relevance_level': get_relevance_level(result.get('score', 0))
        }
        
        # å†…å®¹ã®æ•´ç†ï¼ˆã‚ˆã‚Šè©³ç´°ã«ï¼‰
        content = result.get('content', '')
        formatted_result['summary'] = extract_summary(content, query_analysis)
        formatted_result['full_content'] = content  # åˆ¶é™ã‚’è§£é™¤ã—ã¦å…¨å†…å®¹ã‚’è¡¨ç¤º
        formatted_result['structured_content'] = extract_structured_content(content, query_analysis)
        
        # ä¿®ç†è²»ç”¨ã®æ•´ç†ï¼ˆã‚ˆã‚Šè©³ç´°ã«ï¼‰
        costs = result.get('costs', [])
        formatted_result['repair_costs'] = {
            'items': costs,
            'summary': format_cost_summary(costs),
            'detailed_breakdown': extract_detailed_costs(content)
        }
        
        # æ¨å¥¨è£½å“ã®æ•´ç†ï¼ˆã‚ˆã‚Šè©³ç´°ã«ï¼‰
        alternatives = result.get('alternatives', [])
        formatted_result['recommended_products'] = {
            'items': alternatives,
            'count': len(alternatives),
            'detailed_products': extract_detailed_products(content)
        }
        
        # ä»£ç”¨å“ãƒ»ä»£æ›¿å“ã®æŠ½å‡º
        formatted_result['substitute_products'] = extract_substitute_products(content)
        
        # éƒ¨å“è³¼å…¥æƒ…å ±ã®æŠ½å‡º
        formatted_result['part_purchase_info'] = extract_part_purchase_info(content)
        
        # é–¢é€£URLã®æ•´ç†ï¼ˆã‚ˆã‚Šè©³ç´°ã«ï¼‰
        urls = result.get('urls', [])
        formatted_result['related_links'] = {
            'items': urls,
            'count': len(urls),
            'additional_resources': extract_additional_resources(content)
        }
        
        # ä¿®ç†æ‰‹é †ã®æŠ½å‡º
        formatted_result['repair_steps'] = extract_repair_steps(content)
        
        # æ³¨æ„äº‹é …ãƒ»è­¦å‘Šã®æŠ½å‡º
        formatted_result['warnings'] = extract_warnings(content)
        
        # é–¢é€£åº¦ã®è©³ç´°æƒ…å ±
        formatted_result['relevance_details'] = {
            'main_keywords_matched': get_matched_keywords(content, query_analysis['main_keywords']),
            'context_keywords_matched': get_matched_keywords(content, query_analysis['context_keywords']),
            'is_exact_match': query.lower() in content.lower(),
            'confidence_score': calculate_confidence_score(result, query_analysis)
        }
        
        formatted_results.append(formatted_result)
    
    return formatted_results

def get_relevance_level(score):
    """ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦é–¢é€£åº¦ãƒ¬ãƒ™ãƒ«ã‚’æ±ºå®š"""
    if score >= 100:
        return "high"
    elif score >= 50:
        return "medium"
    elif score >= 20:
        return "low"
    else:
        return "very_low"

def extract_summary(content, query_analysis):
    """å†…å®¹ã‹ã‚‰é–¢é€£ã™ã‚‹éƒ¨åˆ†ã‚’è¦ç´„ã¨ã—ã¦æŠ½å‡º"""
    # ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ã‚’æ¢ã™
    main_keywords = query_analysis['main_keywords']
    context_keywords = query_analysis['context_keywords']
    
    # é–¢é€£ã™ã‚‹æ®µè½ã‚’æŠ½å‡º
    paragraphs = content.split('\n\n')
    relevant_paragraphs = []
    
    for paragraph in paragraphs:
        paragraph_lower = paragraph.lower()
        # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¾ãŸã¯æ–‡è„ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ®µè½ã‚’å„ªå…ˆ
        if any(keyword.lower() in paragraph_lower for keyword in main_keywords + context_keywords):
            relevant_paragraphs.append(paragraph.strip())
    
    # é–¢é€£ã™ã‚‹æ®µè½ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®æ®µè½ã‚’ä½¿ç”¨
    if not relevant_paragraphs:
        relevant_paragraphs = [paragraphs[0]] if paragraphs else [content[:300]]
    
    # è¦ç´„ã‚’ä½œæˆï¼ˆæœ€å¤§300æ–‡å­—ï¼‰
    summary = ' '.join(relevant_paragraphs[:2])
    if len(summary) > 300:
        summary = summary[:300] + "..."
    
    return summary

def format_cost_summary(costs):
    """ä¿®ç†è²»ç”¨ã®è¦ç´„ã‚’ä½œæˆ"""
    if not costs:
        return "è²»ç”¨æƒ…å ±ãªã—"
    
    # è²»ç”¨ã‚’æ•°å€¤ã§ã‚½ãƒ¼ãƒˆ
    numeric_costs = []
    for cost in costs:
        # æ•°å­—ã‚’æŠ½å‡º
        import re
        numbers = re.findall(r'[\d,]+', cost.replace(',', ''))
        if numbers:
            try:
                numeric_costs.append((int(numbers[0].replace(',', '')), cost))
            except:
                numeric_costs.append((0, cost))
    
    if numeric_costs:
        numeric_costs.sort()
        min_cost = numeric_costs[0][1]
        max_cost = numeric_costs[-1][1]
        return f"è²»ç”¨ç›®å®‰: {min_cost} ï½ {max_cost}"
    
    return f"è²»ç”¨ç›®å®‰: {', '.join(costs[:3])}"

def get_matched_keywords(content, keywords):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ãƒãƒƒãƒã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—"""
    content_lower = content.lower()
    matched = []
    for keyword in keywords:
        if keyword.lower() in content_lower:
            matched.append(keyword)
    return matched

def extract_structured_content(content, query_analysis):
    """æ§‹é€ åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
    import re
    
    structured = {
        'problem_description': '',
        'causes': [],
        'solutions': [],
        'tools_needed': [],
        'difficulty_level': '',
        'estimated_time': ''
    }
    
    # é›¨æ¼ã‚Šé–¢é€£ã®ç‰¹åˆ¥ãªå‡¦ç†
    if 'é›¨æ¼ã‚Š' in query_analysis.get('main_keywords', []) or 'é›¨æ¼ã‚Š' in content:
        structured = extract_rain_leak_specific_content(content)
    # ãƒˆã‚¤ãƒ¬é–¢é€£ã®ç‰¹åˆ¥ãªå‡¦ç†
    elif 'ãƒˆã‚¤ãƒ¬' in query_analysis.get('main_keywords', []) or 'ãƒˆã‚¤ãƒ¬' in content:
        structured = extract_toilet_specific_content(content)
    
    # å•é¡Œã®èª¬æ˜ã‚’æŠ½å‡º
    problem_patterns = [
        r'å•é¡Œ[:ï¼š]\s*(.+?)(?:\n|$)',
        r'ç—‡çŠ¶[:ï¼š]\s*(.+?)(?:\n|$)',
        r'æ•…éšœ[:ï¼š]\s*(.+?)(?:\n|$)'
    ]
    
    for pattern in problem_patterns:
        matches = re.findall(pattern, content, re.MULTILINE)
        if matches:
            structured['problem_description'] = matches[0].strip()
            break
    
    # åŸå› ã‚’æŠ½å‡º
    cause_patterns = [
        r'åŸå› [:ï¼š]\s*(.+?)(?:\n\n|\n\d+\.|$)',
        r'è€ƒãˆã‚‰ã‚Œã‚‹åŸå› [:ï¼š]\s*(.+?)(?:\n\n|\n\d+\.|$)',
        r'ä¸»ãªåŸå› [:ï¼š]\s*(.+?)(?:\n\n|\n\d+\.|$)'
    ]
    
    for pattern in cause_patterns:
        matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
        if matches:
            causes_text = matches[0]
            causes = re.split(r'\n[-â€¢]\s*', causes_text)
            structured['causes'] = [cause.strip() for cause in causes if cause.strip()][:5]
            break
    
    # è§£æ±ºç­–ã‚’æŠ½å‡º
    solution_patterns = [
        r'è§£æ±ºæ–¹æ³•[:ï¼š]\s*(.+?)(?:\n\n|\n\d+\.|$)',
        r'ä¿®ç†æ‰‹é †[:ï¼š]\s*(.+?)(?:\n\n|\n\d+\.|$)',
        r'å¯¾å‡¦æ³•[:ï¼š]\s*(.+?)(?:\n\n|\n\d+\.|$)'
    ]
    
    for pattern in solution_patterns:
        matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
        if matches:
            solutions_text = matches[0]
            solutions = re.split(r'\n[-â€¢]\s*', solutions_text)
            structured['solutions'] = [solution.strip() for solution in solutions if solution.strip()][:8]
            break
    
    # å¿…è¦ãªå·¥å…·ã‚’æŠ½å‡º
    tool_patterns = [
        r'å¿…è¦ãªå·¥å…·[:ï¼š]\s*(.+?)(?:\n\n|\n\d+\.|$)',
        r'ä½¿ç”¨å·¥å…·[:ï¼š]\s*(.+?)(?:\n\n|\n\d+\.|$)',
        r'å·¥å…·[:ï¼š]\s*(.+?)(?:\n\n|\n\d+\.|$)'
    ]
    
    for pattern in tool_patterns:
        matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
        if matches:
            tools_text = matches[0]
            tools = re.split(r'\n[-â€¢]\s*', tools_text)
            structured['tools_needed'] = [tool.strip() for tool in tools if tool.strip()][:5]
            break
    
    return structured

def extract_rain_leak_specific_content(content):
    """é›¨æ¼ã‚Šå°‚ç”¨ã®æ§‹é€ åŒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
    import re
    
    structured = {
        'problem_description': 'ã‚­ãƒ£ãƒ³ãƒ”ãƒ³ã‚°ã‚«ãƒ¼ã®é›¨æ¼ã‚Šãƒ»æ°´æ¼ã‚Œãƒˆãƒ©ãƒ–ãƒ«',
        'causes': [],
        'solutions': [],
        'tools_needed': [],
        'difficulty_level': 'ä¸­ç´šã€œä¸Šç´š',
        'estimated_time': '1æ™‚é–“ã€œ1æ—¥'
    }
    
    # é›¨æ¼ã‚Šå°‚ç”¨ã®åŸå› ã‚’æŠ½å‡º
    rain_leak_causes = [
        'ã‚·ãƒ¼ãƒªãƒ³ã‚°ãƒ»ã‚³ãƒ¼ã‚­ãƒ³ã‚°ã®åŠ£åŒ–',
        'ãƒ‘ãƒƒã‚­ãƒ³ã®ç¡¬åŒ–ãƒ»ç ´æ',
        'ãƒ«ãƒ¼ãƒ•ãƒ™ãƒ³ãƒˆå‘¨ã‚Šã®ã‚·ãƒ¼ãƒ«å‰¥ãŒã‚Œ',
        'çª“æ ã®ãƒ¢ãƒ¼ãƒ«éš™é–“',
        'ã‚¢ãƒ³ãƒ†ãƒŠãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ¼ãƒ«å‰²ã‚Œ',
        'é…ç·šå–ã‚Šå‡ºã—éƒ¨ã®é˜²æ°´ä¸è‰¯',
        'FRPã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã®äº€è£‚',
        'æ°´åˆ‡ã‚Šãƒˆãƒ¬ã‚¤ã®è©°ã¾ã‚Š',
        'ãƒ–ãƒ¬ãƒ¼ã‚­ãƒ©ãƒ³ãƒ—ã®ã‚¬ã‚¹ã‚±ãƒƒãƒˆåŠ£åŒ–',
        'çµŒå¹´åŠ£åŒ–ã«ã‚ˆã‚‹é˜²æ°´æã®å‰¥ãŒã‚Œ'
    ]
    
    structured['causes'] = rain_leak_causes
    
    # é›¨æ¼ã‚Šå°‚ç”¨ã®è§£æ±ºæ–¹æ³•ã‚’è¿½åŠ ï¼ˆè²»ç”¨æƒ…å ±ä»˜ãï¼‰
    rain_leak_solutions_with_cost = [
        {
            'title': 'ã‚·ãƒ¼ãƒªãƒ³ã‚°ãƒ»ã‚³ãƒ¼ã‚­ãƒ³ã‚°ã®æ‰“ã¡ç›´ã—',
            'cost': '5,000å††ï½15,000å††',
            'description': 'ãƒ–ãƒãƒ«ãƒ†ãƒ¼ãƒ—ï¼‹ã‚¦ãƒ¬ã‚¿ãƒ³ã‚·ãƒ¼ãƒ©ãƒ³ãƒˆã§ã®è£œä¿®'
        },
        {
            'title': 'ãƒ‘ãƒƒã‚­ãƒ³ãƒ»ã‚¬ã‚¹ã‚±ãƒƒãƒˆã®äº¤æ›',
            'cost': '3,000å††ï½8,000å††',
            'description': 'ç™ºæ³¡ã‚´ãƒ ã‚¬ã‚¹ã‚±ãƒƒãƒˆã¸ã®äº¤æ›'
        },
        {
            'title': 'çª“æ ãƒ¢ãƒ¼ãƒ«ã®é˜²æ°´ã‚·ãƒ¼ãƒ«æ–°èª¿',
            'cost': '8,000å††ï½20,000å††',
            'description': 'çª“æ ã‚’å¤–ã—ã¦ã®é˜²æ°´ã‚·ãƒ¼ãƒ«äº¤æ›'
        },
        {
            'title': 'ãƒ«ãƒ¼ãƒ•ãƒ‘ãƒãƒ«ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã®è£œä¿®',
            'cost': '15,000å††ï½30,000å††',
            'description': 'FRPãƒ‘ãƒ†ã§ã®å†æ¥åˆã¨ã‚³ãƒ¼ã‚­ãƒ³ã‚°'
        },
        {
            'title': 'æ°´åˆ‡ã‚Šãƒˆãƒ¬ã‚¤ã®æ¸…æƒãƒ»ç‚¹æ¤œ',
            'cost': '2,000å††ï½5,000å††',
            'description': 'ãƒ‰ãƒ¬ã‚¤ãƒ³ãƒ›ãƒ¼ã‚¹ã®æ¸…æƒã¨ç‚¹æ¤œ'
        },
        {
            'title': 'ã‚¢ãƒ³ãƒ†ãƒŠãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ¼ãƒ«äº¤æ›',
            'cost': '5,000å††ï½12,000å††',
            'description': 'ãƒ–ãƒãƒ«ï¼‹ã‚¦ãƒ¬ã‚¿ãƒ³ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ãƒ¼ãƒ«'
        },
        {
            'title': 'é…ç·šéƒ¨ã®é˜²æ°´åŠ å·¥',
            'cost': '3,000å††ï½10,000å††',
            'description': 'ã‚°ãƒ­ãƒ¡ãƒƒãƒˆãƒ»ãƒ‘ãƒƒã‚­ãƒ³ã®äº¤æ›'
        },
        {
            'title': 'å¿œæ€¥å‡¦ç½®ï¼ˆãƒ–ãƒãƒ«ãƒ†ãƒ¼ãƒ—ï¼‰',
            'cost': '1,000å††ï½3,000å††',
            'description': 'ç·Šæ€¥é˜²æ°´ãƒ†ãƒ¼ãƒ—ã§ã®å¿œæ€¥å‡¦ç½®'
        }
    ]
    
    structured['solutions'] = rain_leak_solutions_with_cost
    
    # é›¨æ¼ã‚Šå°‚ç”¨ã®å·¥å…·ã‚’è¿½åŠ 
    rain_leak_tools = [
        'ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚»ãƒƒãƒˆ',
        'ã‚³ãƒ¼ã‚­ãƒ³ã‚°ã‚¬ãƒ³',
        'ãƒ–ãƒãƒ«ãƒ†ãƒ¼ãƒ—',
        'ã‚¦ãƒ¬ã‚¿ãƒ³ã‚·ãƒ¼ãƒ©ãƒ³ãƒˆ',
        'é˜²æ°´ã‚¬ã‚¹ã‚±ãƒƒãƒˆ',
        'FRPãƒ‘ãƒ†',
        'ãƒˆãƒƒãƒ—ã‚³ãƒ¼ãƒˆ',
        'æ¸…æƒç”¨ãƒ–ãƒ©ã‚·',
        'ã‚¨ã‚¢ãƒ–ãƒ­ãƒ¼',
        'é˜²æ°´ã‚¹ãƒ—ãƒ¬ãƒ¼',
        'é¤Šç”Ÿãƒ†ãƒ¼ãƒ—',
        'ã‚´ãƒ æ‰‹è¢‹'
    ]
    
    structured['tools_needed'] = rain_leak_tools
    
    return structured

def extract_toilet_specific_content(content):
    """ãƒˆã‚¤ãƒ¬å°‚ç”¨ã®æ§‹é€ åŒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
    import re
    
    structured = {
        'problem_description': 'ã‚­ãƒ£ãƒ³ãƒ”ãƒ³ã‚°ã‚«ãƒ¼ç”¨ãƒˆã‚¤ãƒ¬ï¼ˆã‚«ã‚»ãƒƒãƒˆï¼ãƒãƒªãƒ³ãƒˆã‚¤ãƒ¬ï¼‰ã®ãƒˆãƒ©ãƒ–ãƒ«',
        'causes': [],
        'solutions': [],
        'tools_needed': [],
        'difficulty_level': 'ä¸­ç´š',
        'estimated_time': '30åˆ†ã€œ2æ™‚é–“'
    }
    
    # ãƒˆã‚¤ãƒ¬å°‚ç”¨ã®è§£æ±ºæ–¹æ³•ã‚’è¿½åŠ ï¼ˆè²»ç”¨æƒ…å ±ä»˜ãï¼‰
    toilet_solutions_with_cost = [
        {
            'title': 'ã‚«ã‚»ãƒƒãƒˆã‚¿ãƒ³ã‚¯ã®æ¸…æƒã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹',
            'cost': '0å††ï¼ˆè‡ªåˆ†ã§ä½œæ¥­ï¼‰',
            'description': 'æ¸…æƒç”¨å“ã®ã¿å¿…è¦'
        },
        {
            'title': 'ãƒãƒ³ãƒ—ã®å‹•ä½œç¢ºèªã¨æ¸…æƒ',
            'cost': '3,000å††ï½8,000å††',
            'description': 'ãƒãƒ³ãƒ—æ¸…æƒãƒ»éƒ¨å“äº¤æ›è²»ç”¨'
        },
        {
            'title': 'ã‚·ãƒ¼ãƒ«ãƒ»ãƒ‘ãƒƒã‚­ãƒ³ã®ç‚¹æ¤œã¨äº¤æ›',
            'cost': '5,000å††ï½15,000å††',
            'description': 'éƒ¨å“ä»£ï¼‹å·¥è³ƒ'
        },
        {
            'title': 'é›»æºãƒ»é…ç·šã®ç¢ºèª',
            'cost': '2,000å††ï½10,000å††',
            'description': 'é›»æ°—å·¥äº‹è²»ç”¨'
        },
        {
            'title': 'æ°´ã‚¿ãƒ³ã‚¯ã®æ°´é‡ç¢ºèª',
            'cost': '0å††ï½3,000å††',
            'description': 'çµ¦æ°´ç³»çµ±ã®ç‚¹æ¤œãƒ»ä¿®ç†'
        },
        {
            'title': 'ç•°ç‰©ã®é™¤å»',
            'cost': '1,000å††ï½5,000å††',
            'description': 'æ¸…æƒãƒ»åˆ†è§£ä½œæ¥­è²»ç”¨'
        },
        {
            'title': 'è–¬å‰¤ã®è£œå……',
            'cost': '500å††ï½2,000å††',
            'description': 'è–¬å‰¤ä»£ã®ã¿'
        },
        {
            'title': 'æ’æ°´å¼ã®å‹•ä½œç¢ºèª',
            'cost': '3,000å††ï½8,000å††',
            'description': 'å¼ã®ç‚¹æ¤œãƒ»äº¤æ›è²»ç”¨'
        }
    ]
    
    structured['solutions'] = toilet_solutions_with_cost
    
    # ãƒˆã‚¤ãƒ¬å°‚ç”¨ã®åŸå› ã‚’è¿½åŠ 
    toilet_causes = [
        'ã‚«ã‚»ãƒƒãƒˆã‚¿ãƒ³ã‚¯ã®æ±šã‚Œãƒ»è©°ã¾ã‚Š',
        'ãƒãƒ³ãƒ—ã®æ•…éšœãƒ»è©°ã¾ã‚Š',
        'ã‚·ãƒ¼ãƒ«ãƒ»ãƒ‘ãƒƒã‚­ãƒ³ã®åŠ£åŒ–',
        'é›»æºãƒ»é…ç·šã®å•é¡Œ',
        'æ°´ã‚¿ãƒ³ã‚¯ã®æ°´ä¸è¶³',
        'ç•°ç‰©ã®æ··å…¥',
        'è–¬å‰¤ã®ä¸è¶³',
        'æ’æ°´å¼ã®æ•…éšœ'
    ]
    
    structured['causes'] = toilet_causes
    
    # ãƒˆã‚¤ãƒ¬å°‚ç”¨ã®å·¥å…·ã‚’è¿½åŠ 
    toilet_tools = [
        'ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚»ãƒƒãƒˆ',
        'ãƒãƒ«ãƒãƒ¡ãƒ¼ã‚¿ãƒ¼',
        'ã‚´ãƒ æ‰‹è¢‹',
        'ãƒ–ãƒ©ã‚·ãƒ»ã‚¹ãƒãƒ³ã‚¸',
        'ãƒˆã‚¤ãƒ¬ç”¨æ´—å‰¤',
        'æ–°ã—ã„ã‚·ãƒ¼ãƒ«ãƒ»ãƒ‘ãƒƒã‚­ãƒ³',
        'ãƒˆã‚¤ãƒ¬ç”¨è–¬å‰¤',
        'ã‚¿ã‚ªãƒ«ãƒ»é›‘å·¾'
    ]
    
    structured['tools_needed'] = toilet_tools
    
    return structured

def extract_detailed_costs(content):
    """è©³ç´°ãªè²»ç”¨æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    
    detailed_costs = []
    
    # è²»ç”¨ã®è©³ç´°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šè©³ç´°ãªæ ¹æ‹ ã‚’å«ã‚€ï¼‰
    cost_patterns = [
        # ç¯„å›²æŒ‡å®šã®è²»ç”¨ï¼ˆæ ¹æ‹ ä»˜ãï¼‰
        r'(\d{1,3}(?:,\d{3})*å††)\s*[-~]\s*(\d{1,3}(?:,\d{3})*å††)\s*(.+?)(?:\n|$)',
        r'(\d{1,3}(?:,\d{3})*å††)\s*ï½\s*(\d{1,3}(?:,\d{3})*å††)\s*(.+?)(?:\n|$)',
        # å˜ä¸€è²»ç”¨ï¼ˆæ ¹æ‹ ä»˜ãï¼‰
        r'(\d{1,3}(?:,\d{3})*å††)\s*(.+?)(?:\n|$)',
        # è²»ç”¨é …ç›®åˆ¥
        r'è²»ç”¨[:ï¼š]\s*(\d{1,3}(?:,\d{3})*å††)\s*(.+?)(?:\n|$)',
        # å·¥è³ƒãƒ»éƒ¨å“ä»£åˆ¥
        r'å·¥è³ƒ[:ï¼š]\s*(\d{1,3}(?:,\d{3})*å††)\s*(.+?)(?:\n|$)',
        r'éƒ¨å“ä»£[:ï¼š]\s*(\d{1,3}(?:,\d{3})*å††)\s*(.+?)(?:\n|$)',
        # äº¤æ›è²»ç”¨
        r'äº¤æ›[:ï¼š]\s*(\d{1,3}(?:,\d{3})*å††)\s*(.+?)(?:\n|$)',
        r'ä¿®ç†[:ï¼š]\s*(\d{1,3}(?:,\d{3})*å††)\s*(.+?)(?:\n|$)',
        # é›¨æ¼ã‚Šé–¢é€£ã®è²»ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
        r'ç´„(\d{1,3}(?:,\d{3})*å††)\s*(.+?)(?:\n|$)',
        r'(\d{1,3}(?:,\d{3})*å††)\s*ã§(.+?)(?:\n|$)',
        r'(\d{1,3}(?:,\d{3})*å††)\s*ã»ã©(.+?)(?:\n|$)',
        r'(\d{1,3}(?:,\d{3})*å††)\s*ç¨‹åº¦(.+?)(?:\n|$)',
        # æ™‚é–“å˜ä½ã®è²»ç”¨
        r'(\d{1,3}(?:,\d{3})*å††)\s*ã§(\d+æ™‚é–“|åŠæ—¥|1æ—¥)(.+?)(?:\n|$)',
        r'(\d+æ™‚é–“|åŠæ—¥|1æ—¥)\s*ã§(\d{1,3}(?:,\d{3})*å††)(.+?)(?:\n|$)'
    ]
    
    for pattern in cost_patterns:
        matches = re.findall(pattern, content, re.MULTILINE)
        for match in matches:
            if len(match) == 3:
                detailed_costs.append({
                    'min_cost': match[0],
                    'max_cost': match[1],
                    'description': match[2].strip(),
                    'reason': extract_cost_reason(match[2].strip())
                })
            elif len(match) == 2:
                detailed_costs.append({
                    'cost': match[0],
                    'description': match[1].strip(),
                    'reason': extract_cost_reason(match[1].strip())
                })
    
    return detailed_costs[:5]

def extract_cost_reason(description):
    """è²»ç”¨ã®æ ¹æ‹ ã‚’æŠ½å‡º"""
    import re
    
    reasons = []
    
    # æ ¹æ‹ ã¨ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    reason_patterns = [
        r'(äº¤æ›ãŒå¿…è¦)',
        r'(æ–°å“äº¤æ›)',
        r'(ä¸­å¤å“)',
        r'(éƒ¨å“ä»£ã®ã¿)',
        r'(å·¥è³ƒè¾¼ã¿)',
        r'(å–ã‚Šä»˜ã‘å·¥è³ƒ)',
        r'(è¨ºæ–­è²»ç”¨)',
        r'(ç·Šæ€¥å¯¾å¿œ)',
        r'(å°‚é–€æ¥­è€…)',
        r'(DIYå¯èƒ½)',
        r'(ç°¡å˜äº¤æ›)',
        r'(è¤‡é›‘ãªä¿®ç†)',
        r'(åˆ†è§£ãŒå¿…è¦)',
        r'(å°‚ç”¨å·¥å…·ãŒå¿…è¦)'
    ]
    
    for pattern in reason_patterns:
        matches = re.findall(pattern, description, re.IGNORECASE)
        for match in matches:
            if match not in reasons:
                reasons.append(match)
    
    return reasons

def extract_detailed_products(content):
    """è©³ç´°ãªè£½å“æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    
    detailed_products = []
    
    # è£½å“ã®è©³ç´°ãƒ‘ã‚¿ãƒ¼ãƒ³
    product_patterns = [
        r'ã€([^ã€‘]+)ã€‘\s*[:ï¼š]?\s*(.+?)(?:\n|$)',
        r'ã€Œ([^ã€]+)ã€\s*[:ï¼š]?\s*(.+?)(?:\n|$)',
        r'([A-Za-z0-9\s\-]+å‹)\s*[:ï¼š]?\s*(.+?)(?:\n|$)',
        r'([A-Za-z0-9\s\-]+ã‚·ãƒªãƒ¼ã‚º)\s*[:ï¼š]?\s*(.+?)(?:\n|$)'
    ]
    
    for pattern in product_patterns:
        matches = re.findall(pattern, content, re.MULTILINE)
        for match in matches:
            if len(match) == 2:
                detailed_products.append({
                    'name': match[0].strip(),
                    'description': match[1].strip()
                })
    
    return detailed_products[:5]

def validate_url(url):
    """URLã®æ¤œè¨¼ã¨å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
    if not url:
        return None
    
    import re
    from urllib.parse import urlparse
    
    # åŸºæœ¬çš„ãªURLå½¢å¼ãƒã‚§ãƒƒã‚¯
    if not re.match(r'^https?://', url):
        return None
    
    try:
        parsed = urlparse(url)
        
        # ä¿¡é ¼ã§ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯
        trusted_domains = [
            'amazon.co.jp', 'amazon.com',
            'rakuten.co.jp', 'rakuten.com',
            'yahoo.co.jp', 'yahoo.com',
            'mercari.com', 'auctions.yahoo.co.jp',
            'camper-repair.net', 'titan-rv.com',
            'auto-parts.com', 'parts.com',
            'autoparts.com', 'partssource.com',
            'napaonline.com', 'oreillyauto.com',
            'autozone.com', 'pepboys.com',
            'advanceautoparts.com', 'carquest.com',
            'aap.com', 'worldpac.com',
            'rockauto.com', 'summitracing.com',
            'jegs.com', 'speedwaymotors.com',
            'dennis-kirk.com', 'tuckerrocky.com',
            'partsgiant.com', 'partsgeek.com',
            'autopartswarehouse.com', 'partstrain.com',
            'carparts.com', 'parts.com',
            'autopartscheap.com', 'partzilla.com',
            'boats.net', 'marineengine.com',
            'iboats.com', 'wholesalemarine.com'
        ]
        
        domain = parsed.netloc.lower()
        
        # ä¿¡é ¼ã§ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯
        is_trusted = any(domain.endswith(trusted_domain) for trusted_domain in trusted_domains)
        
        if not is_trusted:
            return None
        
        # URLã®é•·ã•ãƒã‚§ãƒƒã‚¯ï¼ˆç•°å¸¸ã«é•·ã„URLã¯é™¤å¤–ï¼‰
        if len(url) > 500:
            return None
        
        # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        dangerous_patterns = [
            r'javascript:', r'data:', r'vbscript:', r'onload=', r'onerror=',
            r'<script', r'</script>', r'<iframe', r'</iframe>',
            r'\.exe$', r'\.bat$', r'\.cmd$', r'\.scr$', r'\.pif$',
            r'\.com\.exe', r'\.js\.exe', r'\.vbs\.exe'
        ]
        
        for pattern in dangerous_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return None
        
        return url
        
    except Exception:
        return None

def extract_substitute_products(content):
    """ä»£ç”¨å“ãƒ»ä»£æ›¿å“æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    
    substitute_products = []
    
    # ä»£ç”¨å“ãƒ»ä»£æ›¿å“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šåºƒç¯„å›²ã«å¯¾å¿œï¼‰
    substitute_patterns = [
        r'ä»£ç”¨å“[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'ä»£æ›¿å“[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'ä»£æ›¿[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'ä»£ç”¨[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'æ¨å¥¨è£½å“[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'æ¨å¥¨[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'ãŠã™ã™ã‚[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'ãŠå‹§ã‚[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'â€»\s*ä»£ç”¨[å“]*[:ï¼š]\s*(.+?)(?:\n|$)',
        r'â€»\s*ä»£æ›¿[å“]*[:ï¼š]\s*(.+?)(?:\n|$)',
        # ã‚ˆã‚Šåºƒç¯„å›²ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        r'å‚è€ƒ[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'é–¢é€£[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'é¡ä¼¼[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'åŒæ§˜[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        # URLã‚’å«ã‚€è¡Œã®æŠ½å‡º
        r'https?://[^\s]+.*?(?:\n|$)',
        r'www\.[^\s]+.*?(?:\n|$)',
        # è£½å“åã¨URLãŒä¸€ç·’ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆ
        r'([^:ï¼š\n]+)[:ï¼š]\s*(https?://[^\s]+)',
        r'([^:ï¼š\n]+)[:ï¼š]\s*(www\.[^\s]+)'
    ]
    
    for pattern in substitute_patterns:
        matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
        for match in matches:
            # è£½å“åã¨URLã®å‡¦ç†
            if isinstance(match, tuple) and len(match) == 2:
                # è£½å“åã¨URLãŒåˆ†é›¢ã•ã‚Œã¦ã„ã‚‹å ´åˆ
                product_name = match[0].strip()
                url_text = match[1].strip()
                url = validate_url(url_text) if url_text.startswith(('http', 'www')) else None
                
                if product_name and len(product_name) > 2:
                    substitute_products.append({
                        'name': product_name,
                        'url': url,
                        'type': 'substitute'
                    })
            else:
                # è£½å“åã‚’åˆ†å‰²
                products = re.split(r'[,ã€]', match.strip())
                for product in products:
                    product = product.strip()
                    if len(product) > 2 and not product.startswith('â€»'):
                        # URLã‚’å«ã‚€å ´åˆã¯åˆ†é›¢
                        url_match = re.search(r'(https?://[^\s<>"\']+)', product)
                        url = validate_url(url_match.group(1)) if url_match else None
                        product_name = re.sub(r'\s*https?://[^\s<>"\']+', '', product).strip()
                        
                        # è£½å“åãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã®ã¿è¿½åŠ 
                        if product_name and len(product_name) > 2:
                            substitute_products.append({
                                'name': product_name,
                                'url': url,
                                'type': 'substitute'
                            })
    
    # é‡è¤‡ã‚’é™¤å»
    seen = set()
    unique_products = []
    for product in substitute_products:
        if product['name'] not in seen:
            seen.add(product['name'])
            unique_products.append(product)
    
    return unique_products[:8]

def extract_part_purchase_info(content):
    """éƒ¨å“è³¼å…¥æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    
    part_info = []
    
    # éƒ¨å“è³¼å…¥é–¢é€£ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    purchase_patterns = [
        r'è³¼å…¥[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'éƒ¨å“[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'å•†å“[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'è£½å“[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'äº¤æ›éƒ¨å“[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'å¿…è¦ãªéƒ¨å“[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'è³¼å…¥å…ˆ[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'ãŠã™ã™ã‚[:ï¼š]\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'â€»\s*è³¼å…¥[:ï¼š]\s*(.+?)(?:\n|$)',
        r'â€»\s*éƒ¨å“[:ï¼š]\s*(.+?)(?:\n|$)'
    ]
    
    for pattern in purchase_patterns:
        matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
        for match in matches:
            # éƒ¨å“åã‚’åˆ†å‰²
            parts = re.split(r'[,ã€]', match.strip())
            for part in parts:
                part = part.strip()
                if len(part) > 2 and not part.startswith('â€»'):
                    # URLã‚’å«ã‚€å ´åˆã¯åˆ†é›¢
                    url_match = re.search(r'(https?://[^\s<>"\']+)', part)
                    url = validate_url(url_match.group(1)) if url_match else None
                    part_name = re.sub(r'\s*https?://[^\s<>"\']+', '', part).strip()
                    
                    if part_name:
                        part_info.append({
                            'name': part_name,
                            'url': url,
                            'type': 'part'
                        })
    
    # é‡è¤‡ã‚’é™¤å»
    seen = set()
    unique_parts = []
    for part in part_info:
        if part['name'] not in seen:
            seen.add(part['name'])
            unique_parts.append(part)
    
    return unique_parts[:8]

def extract_additional_resources(content):
    """è¿½åŠ ãƒªã‚½ãƒ¼ã‚¹ã‚’æŠ½å‡º"""
    import re
    
    resources = []
    
    # ãƒªã‚½ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
    resource_patterns = [
        r'å‚è€ƒ[:ï¼š]\s*(.+?)(?:\n|$)',
        r'é–¢é€£æƒ…å ±[:ï¼š]\s*(.+?)(?:\n|$)',
        r'è©³ç´°[:ï¼š]\s*(.+?)(?:\n|$)',
        r'ãƒãƒ‹ãƒ¥ã‚¢ãƒ«[:ï¼š]\s*(.+?)(?:\n|$)'
    ]
    
    for pattern in resource_patterns:
        matches = re.findall(pattern, content, re.MULTILINE)
        for match in matches:
            resources.append(match.strip())
    
    return resources[:3]

def extract_repair_steps(content):
    """ä¿®ç†æ‰‹é †ã‚’æŠ½å‡º"""
    import re
    
    steps = []
    
    # æ‰‹é †ãƒ‘ã‚¿ãƒ¼ãƒ³
    step_patterns = [
        r'(\d+\.\s*.+?)(?:\n\d+\.|$)',
        r'æ‰‹é †\d*[:ï¼š]\s*(.+?)(?:\næ‰‹é †|$)',
        r'ã‚¹ãƒ†ãƒƒãƒ—\d*[:ï¼š]\s*(.+?)(?:\nã‚¹ãƒ†ãƒƒãƒ—|$)'
    ]
    
    for pattern in step_patterns:
        matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
        for match in matches:
            steps.append(match.strip())
    
    return steps[:10]

def extract_warnings(content):
    """è­¦å‘Šãƒ»æ³¨æ„äº‹é …ã‚’æŠ½å‡º"""
    import re
    
    warnings = []
    
    # è­¦å‘Šãƒ‘ã‚¿ãƒ¼ãƒ³
    warning_patterns = [
        r'æ³¨æ„[:ï¼š]\s*(.+?)(?:\n\n|\næ³¨æ„|$)',
        r'è­¦å‘Š[:ï¼š]\s*(.+?)(?:\n\n|\nè­¦å‘Š|$)',
        r'å±é™º[:ï¼š]\s*(.+?)(?:\n\n|\nå±é™º|$)',
        r'âš ï¸\s*(.+?)(?:\n|$)',
        r'ğŸš¨\s*(.+?)(?:\n|$)'
    ]
    
    for pattern in warning_patterns:
        matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
        for match in matches:
            warnings.append(match.strip())
    
    return warnings[:5]

def calculate_confidence_score(result, query_analysis):
    """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    score = result.get('score', 0)
    
    # åŸºæœ¬ã‚¹ã‚³ã‚¢
    confidence = min(score / 100, 1.0)
    
    # ã‚¯ã‚¨ãƒªåˆ†æã«åŸºã¥ãèª¿æ•´
    if query_analysis['is_specific_problem']:
        confidence *= 1.2
    if query_analysis['has_action_verb']:
        confidence *= 1.1
    if query_analysis['has_symptom']:
        confidence *= 1.1
    
    return min(confidence, 1.0)

def search_notion_database(query):
    """Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢"""
    if not notion_client:
        return []
    
    try:
        results = []
        query_lower = query.lower()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹IDã®å–å¾—
        node_db_id = os.getenv("NODE_DB_ID") or os.getenv("NOTION_DIAGNOSTIC_DB_ID")
        case_db_id = os.getenv("CASE_DB_ID") or os.getenv("NOTION_REPAIR_CASE_DB_ID")
        item_db_id = os.getenv("ITEM_DB_ID")
        
        # ä¿®ç†ã‚±ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢
        if case_db_id:
            try:
                response = notion_client.databases.query(
                    database_id=case_db_id,
                    page_size=5
                )
                
                for case in response.get("results", []):
                    properties = case.get("properties", {})
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                    title = "ã‚±ãƒ¼ã‚¹æƒ…å ±"
                    if "title" in properties and properties["title"].get("title"):
                        title = properties["title"]["title"][0]["text"]["content"]
                    elif "ã‚¿ã‚¤ãƒˆãƒ«" in properties and properties["ã‚¿ã‚¤ãƒˆãƒ«"].get("title"):
                        title = properties["ã‚¿ã‚¤ãƒˆãƒ«"]["title"][0]["text"]["content"]
                    
                    # èª¬æ˜ã‚’å–å¾—
                    description = ""
                    if "èª¬æ˜" in properties and properties["èª¬æ˜"].get("rich_text"):
                        description = properties["èª¬æ˜"]["rich_text"][0]["text"]["content"]
                    elif "è§£æ±ºæ–¹æ³•" in properties and properties["è§£æ±ºæ–¹æ³•"].get("rich_text"):
                        description = properties["è§£æ±ºæ–¹æ³•"]["rich_text"][0]["text"]["content"]
                    
                    # ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
                    category = ""
                    if "ã‚«ãƒ†ã‚´ãƒª" in properties and properties["ã‚«ãƒ†ã‚´ãƒª"].get("select"):
                        category = properties["ã‚«ãƒ†ã‚´ãƒª"]["select"]["name"]
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                    if (query_lower in title.lower() or 
                        query_lower in description.lower() or
                        query_lower in category.lower()):
                        
                        # å¿…è¦ãªéƒ¨å“ã‚’å–å¾—
                        parts = []
                        if "å¿…è¦ãªéƒ¨å“" in properties and properties["å¿…è¦ãªéƒ¨å“"].get("multi_select"):
                            parts = [item["name"] for item in properties["å¿…è¦ãªéƒ¨å“"]["multi_select"]]
                        
                        # å¿…è¦ãªå·¥å…·ã‚’å–å¾—
                        tools = []
                        if "å¿…è¦ãªå·¥å…·" in properties and properties["å¿…è¦ãªå·¥å…·"].get("multi_select"):
                            tools = [item["name"] for item in properties["å¿…è¦ãªå·¥å…·"]["multi_select"]]
                        
                        results.append({
                            'title': title,
                            'category': category,
                            'description': description,
                            'parts': parts,
                            'tools': tools,
                            'source': 'notion_case'
                        })
            except Exception as e:
                logger.warning(f"ä¿®ç†ã‚±ãƒ¼ã‚¹DBæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # éƒ¨å“ãƒ»å·¥å…·ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢
        if item_db_id:
            try:
                response = notion_client.databases.query(
                    database_id=item_db_id,
                    page_size=3
                )
                
                for item in response.get("results", []):
                    properties = item.get("properties", {})
                    
                    # éƒ¨å“åã‚’å–å¾—
                    item_name = ""
                    if "éƒ¨å“å" in properties and properties["éƒ¨å“å"].get("title"):
                        item_name = properties["éƒ¨å“å"]["title"][0]["text"]["content"]
                    elif "åå‰" in properties and properties["åå‰"].get("title"):
                        item_name = properties["åå‰"]["title"][0]["text"]["content"]
                    
                    # ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
                    category = ""
                    if "ã‚«ãƒ†ã‚´ãƒª" in properties and properties["ã‚«ãƒ†ã‚´ãƒª"].get("select"):
                        category = properties["ã‚«ãƒ†ã‚´ãƒª"]["select"]["name"]
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                    if (query_lower in item_name.lower() or 
                        query_lower in category.lower()):
                        
                        # èª¬æ˜ã‚’å–å¾—
                        description = ""
                        if "èª¬æ˜" in properties and properties["èª¬æ˜"].get("rich_text"):
                            description = properties["èª¬æ˜"]["rich_text"][0]["text"]["content"]
                        
                        # ä¾¡æ ¼ã‚’å–å¾—
                        price = ""
                        if "ä¾¡æ ¼" in properties and properties["ä¾¡æ ¼"].get("number"):
                            price = str(properties["ä¾¡æ ¼"]["number"])
                        
                        results.append({
                            'title': f"æ¨å¥¨éƒ¨å“: {item_name}",
                            'category': category,
                            'description': description,
                            'parts': [item_name],
                            'tools': [],
                            'price': price,
                            'source': 'notion_item'
                        })
            except Exception as e:
                logger.warning(f"éƒ¨å“ãƒ»å·¥å…·DBæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return results[:5]  # æœ€å¤§5ä»¶ã¾ã§è¿”ã™
        
    except Exception as e:
        logger.error(f"Notionæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def search_with_serp_integration(query):
    """SERPæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±å–å¾—"""
    if not serp_system:
        return []
    
    try:
        # SERPæ¤œç´¢ã®å®Ÿè¡Œ
        serp_results = search_with_serp(query, 'comprehensive')
        
        if not serp_results or 'results' not in serp_results:
            return []
        
        # SERPçµæœã‚’APIå½¢å¼ã«å¤‰æ›
        formatted_results = []
        
        for result in serp_results['results']:
            # ä¾¡æ ¼æƒ…å ±ã®æŠ½å‡º
            costs = []
            if result.get('price_info') and result['price_info'].get('price'):
                costs.append(f"{result['price_info']['price']}å††")
            
            # ä»£æ›¿å“æƒ…å ±ã®æŠ½å‡º
            alternatives = []
            if result.get('title'):
                # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰è£½å“åã‚’æŠ½å‡º
                title = result['title']
                # è£½å“åã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
                product_patterns = [
                    r'([A-Za-z0-9\s\-]+)ï¼ˆ[^ï¼‰]+ï¼‰',
                    r'ã€([^ã€‘]+)ã€‘',
                    r'ã€Œ([^ã€]+)ã€',
                    r'([A-Za-z0-9\s\-]+)å‹',
                    r'([A-Za-z0-9\s\-]+)ã‚·ãƒªãƒ¼ã‚º'
                ]
                
                for pattern in product_patterns:
                    matches = re.findall(pattern, title)
                    for match in matches:
                        if isinstance(match, tuple):
                            product_name = match[0].strip()
                        else:
                            product_name = match.strip()
                        
                        if len(product_name) >= 3 and product_name not in alternatives:
                            alternatives.append(product_name)
            
            # URLã®æ¤œè¨¼
            urls = []
            if result.get('url'):
                validated_url = validate_url(result['url'])
                if validated_url:
                    urls.append(validated_url)
            
            formatted_result = {
                'title': f"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±: {result.get('title', 'æ¤œç´¢çµæœ')}",
                'category': "SERPæ¤œç´¢",
                'content': result.get('snippet', ''),
                'costs': costs,
                'alternatives': alternatives[:3],  # æœ€å¤§3ä»¶
                'urls': urls,
                'score': int(result.get('relevance_score', 0.5) * 100),  # 0-100ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›
                'source': 'serp',
                'domain': result.get('domain', ''),
                'search_type': result.get('search_type', 'general_info'),
                'price_info': result.get('price_info', {}),
                'realtime': True  # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±ã®ãƒ•ãƒ©ã‚°
            }
            
            formatted_results.append(formatted_result)
        
        return formatted_results[:5]  # æœ€å¤§5ä»¶ã¾ã§è¿”ã™
        
    except Exception as e:
        logger.error(f"SERPæ¤œç´¢çµ±åˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def search_with_rag(query):
    """RAGã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªæ¤œç´¢"""
    if not rag_db:
        return None
    
    try:
        # RAGã‚·ã‚¹ãƒ†ãƒ ã§æ¤œç´¢å®Ÿè¡Œ
        rag_results = enhanced_rag_retrieve(query, rag_db, max_results=5)
        
        # çµæœã‚’æ•´å½¢
        results = []
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å‡¦ç†
        if rag_results.get('text_file_content'):
            content = rag_results['text_file_content']
            
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒæ§‹é€ åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            structured_data = None
            if isinstance(content, dict) and 'solutions' in content:
                structured_data = content
                content = str(content)  # æ–‡å­—åˆ—ã¨ã—ã¦ã‚‚ä¿æŒ
            
            # ä¿®ç†è²»ç”¨ã¨ä»£æ›¿å“æƒ…å ±ã‚’è©³ç´°æŠ½å‡º
            costs = []
            alternatives = []
            urls = []
            
            # è©³ç´°ãªè²»ç”¨æŠ½å‡º
            cost_patterns = [
                r'(\d+[,ï¼Œ]\d+å††)',  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š
                r'(\d+å††)',  # å˜ç´”ãªå††
                r'(\d+ä¸‡å††)',  # ä¸‡å††
                r'(\d+åƒå††)',  # åƒå††
                r'(\d+[,ï¼Œ]\d+ä¸‡å††)',  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šä¸‡å††
            ]
            for pattern in cost_patterns:
                matches = re.findall(pattern, content)
                costs.extend(matches)
            costs = list(set(costs))[:5]  # é‡è¤‡é™¤å»ã—ã¦æœ€å¤§5ä»¶
            
            # è©³ç´°ãªè£½å“åãƒ»URLæŠ½å‡º
            product_patterns = [
                r'[A-Za-z0-9\s]+ï¼ˆ[^ï¼‰]+ï¼‰',  # æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³
                r'ã€[^ã€‘]+ã€‘',  # ã€ã€‘ã§å›²ã¾ã‚ŒãŸè£½å“å
                r'ã€Œ[^ã€]+ã€',  # ã€Œã€ã§å›²ã¾ã‚ŒãŸè£½å“å
                r'[A-Za-z0-9\s]+å‹',  # å‹ç•ª
                r'[A-Za-z0-9\s]+ã‚·ãƒªãƒ¼ã‚º',  # ã‚·ãƒªãƒ¼ã‚ºå
            ]
            for pattern in product_patterns:
                matches = re.findall(pattern, content)
                alternatives.extend(matches)
            alternatives = list(set(alternatives))[:5]  # é‡è¤‡é™¤å»ã—ã¦æœ€å¤§5ä»¶
            
            # URLæŠ½å‡º
            url_patterns = [
                r'https?://[^\s]+',  # HTTP/HTTPS URL
                r'www\.[^\s]+',  # www URL
            ]
            for pattern in url_patterns:
                matches = re.findall(pattern, content)
                urls.extend(matches)
            urls = list(set(urls))[:3]  # é‡è¤‡é™¤å»ã—ã¦æœ€å¤§3ä»¶
            
            # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚’ä½¿ç”¨
            if structured_data:
                result = {
                    'title': "AIæ¤œç´¢çµæœ - ä¿®ç†ã‚¢ãƒ‰ãƒã‚¤ã‚¹",
                    'category': "RAGæ¤œç´¢",
                    'content': content[:2000],
                    'costs': costs,
                    'alternatives': alternatives,
                    'urls': urls,
                    'score': 100,
                    'structured_content': structured_data
                }
            else:
                result = {
                    'title': "AIæ¤œç´¢çµæœ - ä¿®ç†ã‚¢ãƒ‰ãƒã‚¤ã‚¹",
                    'category': "RAGæ¤œç´¢",
                    'content': content[:2000],
                    'costs': costs,
                    'alternatives': alternatives,
                    'urls': urls,
                    'score': 100
                }
            
            results.append(result)
        
        # ãƒ–ãƒ­ã‚°ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
        if rag_results.get('blog_links'):
            blog_content = []
            blog_urls = []
            for blog in rag_results['blog_links']:
                blog_content.append(f"â€¢ {blog['title']}: {blog['url']}")
                blog_urls.append(blog['url'])
            
            results.append({
                'title': "é–¢é€£ãƒ–ãƒ­ã‚°è¨˜äº‹",
                'category': "ãƒ–ãƒ­ã‚°",
                'content': '\n'.join(blog_content),
                'costs': [],
                'alternatives': [],
                'urls': blog_urls,
                'score': 80
            })
        
        return results
        
    except Exception as e:
        logger.error(f"RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def search_repair_advice(query):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¿®ç†ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æ¤œç´¢ï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œç‰ˆï¼‰"""
    try:
        # å…¨æ¤œç´¢å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ—¢å­˜ã®ãƒªã‚¹ãƒˆï¼‰
        text_files = [
            ("ãƒãƒƒãƒ†ãƒªãƒ¼", "ãƒãƒƒãƒ†ãƒªãƒ¼.txt"),
            ("é›¨æ¼ã‚Š", "é›¨æ¼ã‚Š.txt"),
            ("ã‚¨ã‚¢ã‚³ãƒ³", "ã‚¨ã‚¢ã‚³ãƒ³.txt"),
            ("å†·è”µåº«", "å†·è”µåº«.txt"),
            ("ãƒˆã‚¤ãƒ¬", "ãƒˆã‚¤ãƒ¬.txt"),
            ("ãƒˆã‚¤ãƒ¬ãƒ•ã‚¡ãƒ³", "ãƒ™ãƒ³ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ä»˜ããƒˆã‚¤ãƒ¬ãƒ•ã‚¡ãƒ³ã®æ•…éšœ.txt"),
            ("ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼", "ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼è©³ç´°.txt"),
            ("ãƒ‰ã‚¢ãƒ»çª“", "ãƒ‰ã‚¢ãƒ»çª“ã®é–‹é–‰ä¸è‰¯.txt"),
            ("ãƒ’ãƒ¥ãƒ¼ã‚ºãƒ»ãƒªãƒ¬ãƒ¼", "ãƒ’ãƒ¥ãƒ¼ã‚ºåˆ‡ã‚Œãƒ»ãƒªãƒ¬ãƒ¼ä¸è‰¯.txt"),
            ("ã‚¬ã‚¹ã‚³ãƒ³ãƒ­", "ã‚¬ã‚¹ã‚³ãƒ³ãƒ­.txt"),
            ("æ°´é“ãƒãƒ³ãƒ—", "æ°´é“ãƒãƒ³ãƒ—.txt"),
            ("ã‚½ãƒ¼ãƒ©ãƒ¼ãƒ‘ãƒãƒ«", "ã‚½ãƒ¼ãƒ©ãƒ¼ãƒ‘ãƒãƒ«.txt"),
            ("è»Šä½“å¤–è£…", "è»Šä½“å¤–è£…ã®ç ´æ.txt"),
            ("ã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼", "ã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼.txt"),
            ("ã‚¿ã‚¤ãƒ¤", "ã‚­ãƒ£ãƒ³ãƒ”ãƒ³ã‚°ã‚«ãƒ¼ã€€ã‚¿ã‚¤ãƒ¤ã€€.txt"),
            ("é›»è£…ç³»", "é›»è£…ç³».txt"),
            ("FFãƒ’ãƒ¼ã‚¿ãƒ¼", "FFãƒ’ãƒ¼ã‚¿ãƒ¼.txt"),
            ("ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦", "ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦.txt"),
            ("ãƒ«ãƒ¼ãƒ•ãƒ™ãƒ³ãƒˆ", "ãƒ«ãƒ¼ãƒ•ãƒ™ãƒ³ãƒˆã€€æ›æ°—æ‰‡.txt"),
            ("å¤–éƒ¨é›»æº", "å¤–éƒ¨é›»æº.txt"),
            ("å®¤å†…LED", "å®¤å†…LED.txt"),
            ("å®¶å…·", "å®¶å…·.txt"),
            ("æ’æ°´ã‚¿ãƒ³ã‚¯", "æ’æ°´ã‚¿ãƒ³ã‚¯.txt"),
            ("ç•°éŸ³", "ç•°éŸ³.txt")
        ]
        
        # ã‚¯ã‚¨ãƒªè§£æçµæœã‚’å–å¾—
        query_analysis = analyze_query(query_lower)
        
        # ãƒ‰ã‚¢é–¢é€£ã®ã‚¯ã‚¨ãƒªã®å ´åˆã¯ã€ãƒ‰ã‚¢é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆçš„ã«æ¤œç´¢
        if ('ãƒ‰ã‚¢' in query_analysis.get('main_keywords', []) or 
            'çª“' in query_analysis.get('main_keywords', []) or 
            'é–‹é–‰' in query_analysis.get('main_keywords', [])):
            # ãƒ‰ã‚¢é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ˆé ­ã«ç§»å‹•
            door_files = [f for f in text_files if 'ãƒ‰ã‚¢' in f[0] or 'çª“' in f[0] or 'ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦' in f[0]]
            other_files = [f for f in text_files if f not in door_files]
            text_files = door_files + other_files
            logger.info(f"ãƒ‰ã‚¢é–¢é€£ã‚¯ã‚¨ãƒªã®ãŸã‚ã€ãƒ‰ã‚¢é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆ: {[f[1] for f in door_files]}")
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ¤œç´¢å¯¾è±¡ã«è¿½åŠ 
        markdown_files = glob.glob("*.md")
        for md_file in markdown_files:
            if not any(f[1] == md_file for f in text_files):
                category_name = md_file.replace(".md", "").replace("_", "ãƒ»")
                text_files.append((category_name, md_file))
        
        # è¿½åŠ ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‹•çš„ã«æ¤œç´¢
        additional_files = glob.glob("*.txt")
        for filename in additional_files:
            # æ—¢å­˜ãƒªã‚¹ãƒˆã«ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
            if not any(f[1] == filename for f in text_files):
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªåã‚’æ¨æ¸¬
                category_name = filename.replace(".txt", "").replace("ã€€", "ãƒ»")
                text_files.append((category_name, filename))
        
        results = []
        query_lower = query.lower()
        
        logger.info(f"æ¤œç´¢å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(text_files)}")
        logger.info(f"æ¤œç´¢å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {[f[1] for f in text_files[:5]]}")  # æœ€åˆã®5ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ã‚°å‡ºåŠ›
        
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        for category, filename in text_files:
            try:
                if not os.path.exists(filename):
                    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {filename}")
                    continue
                    
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {filename}")
                with open(filename, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(content)} æ–‡å­—")
                
                # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è§£æï¼ˆ.mdãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼‰
                if filename.endswith('.md'):
                    content = parse_markdown_content(content)
                
                # ã‚¯ã‚¨ãƒªè§£æçµæœã¯æ—¢ã«å–å¾—æ¸ˆã¿
                
                # é«˜åº¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                matches = []
                query_words = [word.strip() for word in query_lower.split() if len(word.strip()) > 1]
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã®é–¢é€£æ€§ã‚’äº‹å‰ãƒã‚§ãƒƒã‚¯
                file_relevance = 0
                filename_lower = filename.lower()
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯
                if any(keyword in filename_lower for keyword in query_analysis['main_keywords']):
                    file_relevance += 30
                
                # ã‚«ãƒ†ã‚´ãƒªã¨ã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯
                if any(keyword in category.lower() for keyword in query_analysis['main_keywords']):
                    file_relevance += 25
                
                # é–¢é€£æ€§ãŒä½ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if file_relevance < 10 and not query_analysis['is_specific_problem']:
                    continue
                
                # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å®Œå…¨ä¸€è‡´ï¼ˆé«˜å„ªå…ˆåº¦ï¼‰
                for keyword in query_analysis['main_keywords']:
                    if keyword in content.lower():
                        matches.append(('main_keyword', keyword, 40))
                
                # æ–‡è„ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€è‡´
                for keyword in query_analysis['context_keywords']:
                    if keyword in content.lower():
                        matches.append(('context_keyword', keyword, 25))
                
                # å®Œå…¨ä¸€è‡´ï¼ˆæœ€é«˜å„ªå…ˆåº¦ï¼‰
                if query_lower in content.lower():
                    matches.append(('exact', query_lower, 60))
                
                # éƒ¨åˆ†ä¸€è‡´
                for word in query_words:
                    if len(word) >= 3 and word in content.lower():
                        matches.append(('partial', word, 20))
                
                # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                related_keywords = get_related_keywords(query_lower)
                for keyword in related_keywords:
                    if keyword in content.lower():
                        matches.append(('related', keyword, 10))
                
                # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒãƒ³ã‚°
                if any(keyword in category.lower() for keyword in query_analysis['main_keywords']):
                    matches.append(('category', category, 30))
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åãƒãƒƒãƒãƒ³ã‚°
                if any(keyword in filename_lower for keyword in query_analysis['main_keywords']):
                    matches.append(('filename', filename, 25))
                
                if matches:
                    logger.info(f"ãƒãƒƒãƒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {len(matches)}ä»¶")
                    # é–¢é€£åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆæ”¹å–„ç‰ˆï¼‰
                    score = file_relevance  # ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’ãƒ™ãƒ¼ã‚¹ã«
                    
                    for match_type, keyword, weight in matches:
                        count = content.lower().count(keyword) if isinstance(keyword, str) else 1
                        score += count * weight
                        logger.info(f"  ãƒãƒƒãƒ: {keyword} ({count}å›) +{count * weight}ç‚¹")
                    
                    # ã‚¯ã‚¨ãƒªè§£æã«åŸºã¥ããƒœãƒ¼ãƒŠã‚¹
                    if query_analysis['is_specific_problem']:
                        score *= 2.0  # å…·ä½“çš„ãªå•é¡Œã®å ´åˆã¯å¤§å¹…ãƒœãƒ¼ãƒŠã‚¹
                    
                    if query_analysis['has_action_verb']:
                        score *= 1.3  # å‹•ä½œå‹•è©ãŒã‚ã‚‹å ´åˆã¯ãƒœãƒ¼ãƒŠã‚¹
                    
                    if query_analysis['has_symptom']:
                        score *= 1.2  # ç—‡çŠ¶ãŒã‚ã‚‹å ´åˆã¯ãƒœãƒ¼ãƒŠã‚¹
                    
                    # å†…å®¹ã®é•·ã•ã«ã‚ˆã‚‹æ­£è¦åŒ–
                    if len(content) > 1000:
                        score *= 0.9
                    elif len(content) < 200:
                        score *= 0.7  # å†…å®¹ãŒçŸ­ã™ãã‚‹å ´åˆã¯æ¸›ç‚¹
                    
                    # ä¿®ç†è²»ç”¨ã¨ä»£æ›¿å“æƒ…å ±ã‚’è©³ç´°æŠ½å‡º
                    costs = []
                    alternatives = []
                    urls = []
                    
                    # è©³ç´°ãªè²»ç”¨æŠ½å‡º
                    cost_patterns = [
                        r'(\d+[,ï¼Œ]\d+å††)',  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š
                        r'(\d+å††)',  # å˜ç´”ãªå††
                        r'(\d+ä¸‡å††)',  # ä¸‡å††
                        r'(\d+åƒå††)',  # åƒå††
                        r'(\d+[,ï¼Œ]\d+ä¸‡å††)',  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šä¸‡å††
                    ]
                    for pattern in cost_patterns:
                        matches = re.findall(pattern, content)
                        costs.extend(matches)
                    costs = list(set(costs))[:5]  # é‡è¤‡é™¤å»ã—ã¦æœ€å¤§5ä»¶
                    
                    # è£½å“åãƒ»ä»£æ›¿å“æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
                    product_patterns = [
                        r'ã€([^ã€‘]+)ã€‘',  # ã€ã€‘ã§å›²ã¾ã‚ŒãŸè£½å“å
                        r'ã€Œ([^ã€]+)ã€',  # ã€Œã€ã§å›²ã¾ã‚ŒãŸè£½å“å
                        r'([A-Za-z0-9\s\-]+)ï¼ˆ[^ï¼‰]+ï¼‰',  # ï¼ˆï¼‰ã§å›²ã¾ã‚ŒãŸè£½å“å
                        r'([A-Za-z0-9\s\-]+)å‹',  # å‹ç•ª
                        r'([A-Za-z0-9\s\-]+)ã‚·ãƒªãƒ¼ã‚º',  # ã‚·ãƒªãƒ¼ã‚ºå
                        r'([A-Za-z0-9\s\-]+)ã‚¨ã‚¢ã‚³ãƒ³',  # ã‚¨ã‚¢ã‚³ãƒ³è£½å“
                        r'([A-Za-z0-9\s\-]+)ãƒãƒƒãƒ†ãƒªãƒ¼',  # ãƒãƒƒãƒ†ãƒªãƒ¼è£½å“
                        r'([A-Za-z0-9\s\-]+)ãƒ•ã‚¡ãƒ³',  # ãƒ•ã‚¡ãƒ³è£½å“
                    ]
                    for pattern in product_patterns:
                        matches = re.findall(pattern, content)
                        for match in matches:
                            # è£½å“åã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                            if isinstance(match, tuple):
                                product_name = match[0].strip()
                            else:
                                product_name = match.strip()
                            
                            # ç„¡åŠ¹ãªæ–‡å­—ã‚’é™¤å¤–
                            if (len(product_name) >= 3 and 
                                not product_name.startswith('Case') and
                                not product_name.startswith('ã€') and
                                not product_name.startswith('ã€Œ') and
                                not any(char in product_name for char in ['ãƒ»', 'ã€', 'ï¼Œ', 'ï¼›'])):
                                alternatives.append(product_name)
                    
                    # é‡è¤‡é™¤å»ã¨æœ€å¤§ä»¶æ•°åˆ¶é™
                    alternatives = list(dict.fromkeys(alternatives))[:5]  # é †åºã‚’ä¿æŒã—ã¦é‡è¤‡é™¤å»
                    
                    # URLæŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
                    url_patterns = [
                        r'https?://[^\s<>"{}|\\^`\[\]]+',  # HTTP/HTTPS URL
                        r'www\.[^\s<>"{}|\\^`\[\]]+\.[a-zA-Z]{2,}',  # www URL
                    ]
                    for pattern in url_patterns:
                        url_matches = re.findall(pattern, content)
                        # URLã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                        for url in url_matches:
                            # ä¸æ­£ãªæ–‡å­—ã‚„è¨˜å·ã‚’é™¤å»
                            url = url.strip('.,;!?')
                            
                            # www URLã®å ´åˆã¯https://ã‚’è¿½åŠ 
                            if url.startswith('www.'):
                                url = 'https://' + url
                            
                            # URLæ¤œè¨¼ã‚’é©ç”¨
                            validated_url = validate_url(url)
                            if validated_url:
                                urls.append(validated_url)
                    
                    # é‡è¤‡é™¤å»ã¨æœ€å¤§ä»¶æ•°åˆ¶é™
                    urls = list(dict.fromkeys(urls))[:3]  # é †åºã‚’ä¿æŒã—ã¦é‡è¤‡é™¤å»
                    
                    results.append({
                        'title': f"{category}ä¿®ç†ã‚¢ãƒ‰ãƒã‚¤ã‚¹",
                        'category': category,
                        'filename': filename,
                        'content': content[:1500],  # å†…å®¹ã‚’1500æ–‡å­—ã«æ‹¡å¼µ
                        'costs': costs,
                        'alternatives': alternatives,
                        'urls': urls,
                        'score': score
                    })
                    
                    # 8ä»¶è¦‹ã¤ã‹ã£ãŸã‚‰çµ‚äº†ï¼ˆã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’æä¾›ï¼‰
                    if len(results) >= 8:
                        break
            except Exception as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ« {filename} ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x['score'], reverse=True)
        
        return results
        
    except Exception as e:
        logger.error(f"æ¤œç´¢å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def get_general_repair_advice(query):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ãä¸€èˆ¬çš„ãªä¿®ç†ã‚¢ãƒ‰ãƒã‚¤ã‚¹"""
    query_lower = query.lower()
    
    advice_map = {
        'ãƒãƒƒãƒ†ãƒªãƒ¼': """
**ãƒãƒƒãƒ†ãƒªãƒ¼é–¢é€£ã®ä¸€èˆ¬çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹:**
- å®šæœŸçš„ãªé›»åœ§ãƒã‚§ãƒƒã‚¯ï¼ˆ12.6Vä»¥ä¸ŠãŒæ­£å¸¸ï¼‰
- å……é›»å™¨ã®é©åˆ‡ãªä½¿ç”¨
- éæ”¾é›»ã®é˜²æ­¢
- ç«¯å­ã®æ¸…æƒã¨ç‚¹æ¤œ
- 2-3å¹´ã§ã®äº¤æ›ã‚’æ¨å¥¨
        """,
        'é›¨æ¼ã‚Š': """
**é›¨æ¼ã‚Šé–¢é€£ã®ä¸€èˆ¬çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹:**
- ã‚·ãƒ¼ãƒªãƒ³ã‚°æã®å®šæœŸç‚¹æ¤œ
- æ—©æœŸç™ºè¦‹ã¨å¿œæ€¥å‡¦ç½®
- å°‚é–€æ¥­è€…ã«ã‚ˆã‚‹æ ¹æœ¬çš„ãªä¿®ç†
- é˜²æ°´ãƒ†ãƒ¼ãƒ—ã§ã®å¿œæ€¥å‡¦ç½®
- å®šæœŸçš„ãªãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹
        """,
        'ã‚¨ã‚¢ã‚³ãƒ³': """
**ã‚¨ã‚¢ã‚³ãƒ³é–¢é€£ã®ä¸€èˆ¬çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹:**
- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å®šæœŸæ¸…æƒ
- å†·åª’ã‚¬ã‚¹ã®ç‚¹æ¤œ
- å®¤å¤–æ©Ÿã®æ¸…æƒ
- é©åˆ‡ãªæ¸©åº¦è¨­å®š
- å°‚é–€æ¥­è€…ã«ã‚ˆã‚‹å®šæœŸç‚¹æ¤œ
        """,
        'ãƒˆã‚¤ãƒ¬': """
**ãƒˆã‚¤ãƒ¬é–¢é€£ã®ä¸€èˆ¬çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹:**
- å®šæœŸçš„ãªæ¸…æƒã¨æ¶ˆæ¯’
- æ°´ã‚¿ãƒ³ã‚¯ã®ç‚¹æ¤œ
- ãƒ‘ãƒƒã‚­ãƒ³ã®äº¤æ›
- è‡­æ°—å¯¾ç­–
- å°‚é–€æ¥­è€…ã«ã‚ˆã‚‹å®šæœŸãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹
        """
    }
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
    for keyword, advice in advice_map.items():
        if keyword in query_lower:
            return advice
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹
    return """
**ä¸€èˆ¬çš„ãªä¿®ç†ã‚¢ãƒ‰ãƒã‚¤ã‚¹:**
- å®šæœŸçš„ãªç‚¹æ¤œã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹
- æ—©æœŸç™ºè¦‹ã¨é©åˆ‡ãªå¯¾å¿œ
- å°‚é–€æ¥­è€…ã¸ã®ç›¸è«‡
- å®‰å…¨ç¬¬ä¸€ã®ä½œæ¥­
- é©åˆ‡ãªå·¥å…·ã¨éƒ¨å“ã®ä½¿ç”¨
    """

@app.route('/')
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ï¼ˆHTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™ï¼‰"""
    try:
        with open('repair_advice_center.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        return html_content
    except FileNotFoundError:
        return "HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", 404

@app.route('/api/search', methods=['POST'])
def api_search():
    """æ¤œç´¢API"""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        
        if not query:
            return jsonify({
                'success': False,
                'error': 'æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'
            }), 400
        
        logger.info(f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")
        
        # ã‚¯ã‚¨ãƒªè§£æçµæœã‚’ãƒ­ã‚°å‡ºåŠ›
        query_analysis = analyze_query(query)
        logger.info(f"ã‚¯ã‚¨ãƒªè§£æçµæœ: {query_analysis}")
        
        # çµ±åˆæ¤œç´¢ã®å®Ÿè¡Œ
        all_results = []
        
        # 1. RAGã‚·ã‚¹ãƒ†ãƒ ã§æ¤œç´¢ã‚’è©¦è¡Œ
        rag_results = search_with_rag(query)
        if rag_results:
            all_results.extend(rag_results)
            logger.info(f"RAGæ¤œç´¢ã§{len(rag_results)}ä»¶ã®çµæœã‚’å–å¾—")
        
        # 2. Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
        notion_results = search_notion_database(query)
        if notion_results:
            # Notionçµæœã‚’APIå½¢å¼ã«å¤‰æ›
            for notion_result in notion_results:
                all_results.append({
                    'title': notion_result['title'],
                    'category': notion_result['category'],
                    'content': notion_result['description'],
                    'costs': [notion_result.get('price', '')] if notion_result.get('price') else [],
                    'alternatives': notion_result.get('parts', []),
                    'urls': [],
                    'score': 90,  # Notionçµæœã¯é«˜ã‚¹ã‚³ã‚¢
                    'source': 'notion'
                })
            logger.info(f"Notionæ¤œç´¢ã§{len(notion_results)}ä»¶ã®çµæœã‚’å–å¾—")
        
        # 3. SERPæ¤œç´¢ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±å–å¾—ï¼‰
        serp_results = search_with_serp_integration(query)
        if serp_results:
            all_results.extend(serp_results)
            logger.info(f"SERPæ¤œç´¢ã§{len(serp_results)}ä»¶ã®çµæœã‚’å–å¾—")
        
        # 4. å¾“æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        # ãƒ‰ã‚¢é–¢é€£ã®ã‚¯ã‚¨ãƒªã®å ´åˆã¯å¿…ãšãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã‚’å®Ÿè¡Œ
        if len(all_results) < 3 or ('ãƒ‰ã‚¢' in query_analysis.get('main_keywords', []) or 'çª“' in query_analysis.get('main_keywords', []) or 'é–‹é–‰' in query_analysis.get('main_keywords', [])):
            logger.info("çµæœãŒä¸ååˆ†ãªãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™")
            text_results = search_repair_advice(query)
            if text_results:
                all_results.extend(text_results)
                logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã§{len(text_results)}ä»¶ã®çµæœã‚’å–å¾—")
                # å„çµæœã®ã‚¹ã‚³ã‚¢ã‚’ãƒ­ã‚°å‡ºåŠ›
                for i, result in enumerate(text_results):
                    logger.info(f"  çµæœ{i+1}: {result.get('title', 'N/A')} (ã‚¹ã‚³ã‚¢: {result.get('score', 0)})")
            else:
                logger.info("ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã§ã‚‚çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # çµæœã‚’ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results = sorted(all_results, key=lambda x: x.get('score', 0), reverse=True)
        logger.info(f"ã‚½ãƒ¼ãƒˆå¾Œã®å…¨çµæœæ•°: {len(results)}")
        
        # çµæœã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨æ”¹å–„ï¼ˆå³å¯†ç‰ˆï¼‰
        filtered_results = []
        seen_titles = set()
        
        # ã‚¯ã‚¨ãƒªè§£æçµæœã¯æ—¢ã«å–å¾—æ¸ˆã¿
        
        for i, result in enumerate(results):
            logger.info(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ä¸­: çµæœ{i+1} - {result.get('title', 'N/A')} (ã‚¹ã‚³ã‚¢: {result.get('score', 0)})")
            
            # é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«ã®é™¤å»
            title = result.get('title', '')
            if title in seen_titles:
                logger.info(f"  é‡è¤‡ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {title}")
                continue
            seen_titles.add(title)
            
            # ã‚¹ã‚³ã‚¢é–¾å€¤ã‚’ã‚¯ã‚¨ãƒªè§£æã«åŸºã¥ã„ã¦å‹•çš„ã«è¨­å®šï¼ˆãƒ‰ã‚¢é–¢é€£ã¯éå¸¸ã«ç·©ãï¼‰
            if 'ãƒ‰ã‚¢' in query_analysis.get('main_keywords', []) or 'çª“' in query_analysis.get('main_keywords', []) or 'é–‹é–‰' in query_analysis.get('main_keywords', []):
                min_score_threshold = 10  # ãƒ‰ã‚¢é–¢é€£ã¯é–¾å€¤ã‚’éå¸¸ã«ä¸‹ã’ã‚‹
            else:
                min_score_threshold = 50 if query_analysis['is_specific_problem'] else 40
            
            # ä½ã‚¹ã‚³ã‚¢çµæœã®é™¤å¤–
            if result.get('score', 0) < min_score_threshold:
                logger.info(f"  ã‚¹ã‚³ã‚¢ä¸è¶³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {result.get('score', 0)} < {min_score_threshold}")
                continue
            
            # å†…å®¹ã®é•·ã•ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‰ã‚¢é–¢é€£ã¯éå¸¸ã«ç·©ãï¼‰
            content = result.get('content', '')
            if 'ãƒ‰ã‚¢' in query_analysis.get('main_keywords', []) or 'çª“' in query_analysis.get('main_keywords', []) or 'é–‹é–‰' in query_analysis.get('main_keywords', []):
                if len(content) < 50:  # ãƒ‰ã‚¢é–¢é€£ã¯50æ–‡å­—ä»¥ä¸Š
                    logger.info(f"  å†…å®¹ãŒçŸ­ã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {len(content)}æ–‡å­—")
                    continue
            else:
                if len(content) < 300:  # ãã®ä»–ã¯300æ–‡å­—ä»¥ä¸Š
                    continue
            
            # ã‚«ãƒ†ã‚´ãƒªã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‰ã‚¢é–¢é€£ã¯éå¸¸ã«ç·©ãï¼‰
            category = result.get('category', '').lower()
            if query_analysis['main_keywords']:
                if not any(keyword.lower() in category for keyword in query_analysis['main_keywords']):
                    # ãƒ‰ã‚¢é–¢é€£ã®å ´åˆã¯ã‚¹ã‚³ã‚¢ãŒé«˜ã‘ã‚Œã°é€šã™
                    if 'ãƒ‰ã‚¢' in query_analysis.get('main_keywords', []) or 'çª“' in query_analysis.get('main_keywords', []) or 'é–‹é–‰' in query_analysis.get('main_keywords', []):
                        if result.get('score', 0) < 20:  # ãƒ‰ã‚¢é–¢é€£ã¯20ç‚¹ä»¥ä¸Š
                            logger.info(f"  ã‚«ãƒ†ã‚´ãƒªé–¢é€£æ€§ä¸è¶³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {category}")
                            continue
                    else:
                        # ã‚«ãƒ†ã‚´ãƒªãŒé–¢é€£ã—ã¦ã„ãªã„å ´åˆã¯ã€ã‚¹ã‚³ã‚¢ãŒé«˜ããªã„é™ã‚Šé™¤å¤–
                        if result.get('score', 0) < 80:
                            continue
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‰ã‚¢é–¢é€£ã¯éå¸¸ã«ç·©ãï¼‰
            filename = result.get('filename', '').lower()
            if query_analysis['main_keywords']:
                if not any(keyword.lower() in filename for keyword in query_analysis['main_keywords']):
                    # ãƒ‰ã‚¢é–¢é€£ã®å ´åˆã¯ã‚¹ã‚³ã‚¢ãŒé«˜ã‘ã‚Œã°é€šã™
                    if 'ãƒ‰ã‚¢' in query_analysis.get('main_keywords', []) or 'çª“' in query_analysis.get('main_keywords', []) or 'é–‹é–‰' in query_analysis.get('main_keywords', []):
                        if result.get('score', 0) < 15:  # ãƒ‰ã‚¢é–¢é€£ã¯15ç‚¹ä»¥ä¸Š
                            logger.info(f"  ãƒ•ã‚¡ã‚¤ãƒ«åé–¢é€£æ€§ä¸è¶³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {filename}")
                            continue
                    else:
                        # ãƒ•ã‚¡ã‚¤ãƒ«åãŒé–¢é€£ã—ã¦ã„ãªã„å ´åˆã¯ã€ã‚¹ã‚³ã‚¢ãŒé«˜ããªã„é™ã‚Šé™¤å¤–
                        if result.get('score', 0) < 70:
                            continue
            
            # çµæœã®æ”¹å–„
            if result.get('alternatives'):
                # ä»£æ›¿å“ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                result['alternatives'] = [alt for alt in result['alternatives'] 
                                        if len(alt) >= 3 and not alt.startswith('Case')]
            
            if result.get('urls'):
                # URLã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                result['urls'] = [url for url in result['urls'] 
                                if url and not url.endswith('**') and len(url) > 10]
            
            filtered_results.append(result)
            
            # ãƒ‰ã‚¢é–¢é€£ã®å ´åˆã¯æœ€å¤§3ä»¶ã¾ã§è¿”ã™
            if 'ãƒ‰ã‚¢' in query_analysis.get('main_keywords', []) or 'çª“' in query_analysis.get('main_keywords', []) or 'é–‹é–‰' in query_analysis.get('main_keywords', []):
                if len(filtered_results) >= 3:
                    break
            else:
                # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚‚ã®1ã¤ã ã‘ã«åˆ¶é™
                break
        
        results = filtered_results
        
        if results:
            # çµæœã‚’æ•´ç†ã—ã¦è¡¨ç¤ºç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_results = format_search_results(results, query)
            
            return jsonify({
                'success': True,
                'results': formatted_results,
                'count': len(formatted_results),
                'query_info': {
                    'original_query': query,
                    'analysis': analyze_query(query),
                    'total_found': len(all_results),
                    'filtered_count': len(formatted_results)
                }
            })
        else:
            # çµæœãŒãªã„å ´åˆã®ä¸€èˆ¬çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹
            general_advice = get_general_repair_advice(query)
            return jsonify({
                'success': True,
                'results': [],
                'general_advice': general_advice,
                'count': 0
            })
            
    except Exception as e:
        logger.error(f"APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }), 500

@app.route('/api/serp-search', methods=['POST'])
def serp_search_api():
    """SERPæ¤œç´¢å°‚ç”¨API"""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        search_type = data.get('search_type', 'comprehensive')
        
        if not query:
            return jsonify({
                'success': False,
                'error': 'æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'
            }), 400
        
        if not serp_system:
            return jsonify({
                'success': False,
                'error': 'SERPæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚'
            }), 503
        
        logger.info(f"SERPæ¤œç´¢ã‚¯ã‚¨ãƒª: {query} (ã‚¿ã‚¤ãƒ—: {search_type})")
        
        # SERPæ¤œç´¢ã®å®Ÿè¡Œ
        serp_results = search_with_serp(query, search_type)
        
        if serp_results and 'results' in serp_results:
            # çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_results = []
            
            for result in serp_results['results']:
                formatted_result = {
                    'title': result.get('title', ''),
                    'url': result.get('url', ''),
                    'snippet': result.get('snippet', ''),
                    'domain': result.get('domain', ''),
                    'relevance_score': result.get('relevance_score', 0),
                    'search_type': result.get('search_type', ''),
                    'price_info': result.get('price_info', {}),
                    'source': result.get('source', '')
                }
                formatted_results.append(formatted_result)
            
            return jsonify({
                'success': True,
                'results': formatted_results,
                'query_info': {
                    'original_query': query,
                    'search_type': search_type,
                    'intent_analysis': serp_results.get('intent_analysis', {}),
                    'total_found': len(formatted_results),
                    'search_engines_used': serp_results.get('search_engines_used', [])
                }
            })
        else:
            return jsonify({
                'success': True,
                'results': [],
                'message': 'æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚'
            })
            
    except Exception as e:
        logger.error(f"SERPæ¤œç´¢APIã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'SERPæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }), 500

@app.route('/api/realtime-info', methods=['POST'])
def realtime_info_api():
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±å–å¾—API"""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        
        if not query:
            return jsonify({
                'success': False,
                'error': 'æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'
            }), 400
        
        if not serp_system:
            return jsonify({
                'success': False,
                'error': 'SERPæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚'
            }), 503
        
        logger.info(f"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±æ¤œç´¢: {query}")
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¿®ç†æƒ…å ±ã®å–å¾—
        realtime_results = serp_system.get_realtime_repair_info(query)
        
        if realtime_results and 'results' in realtime_results:
            return jsonify({
                'success': True,
                'results': realtime_results['results'],
                'query_info': realtime_results.get('query_info', {}),
                'intent_analysis': realtime_results.get('intent_analysis', {})
            })
        else:
            return jsonify({
                'success': True,
                'results': [],
                'message': 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚'
            })
            
    except Exception as e:
        logger.error(f"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±APIã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }), 500

@app.route('/api/parts-price', methods=['POST'])
def parts_price_api():
    """éƒ¨å“ä¾¡æ ¼æ¤œç´¢API"""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        
        if not query:
            return jsonify({
                'success': False,
                'error': 'æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'
            }), 400
        
        if not serp_system:
            return jsonify({
                'success': False,
                'error': 'SERPæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚'
            }), 503
        
        logger.info(f"éƒ¨å“ä¾¡æ ¼æ¤œç´¢: {query}")
        
        # éƒ¨å“ä¾¡æ ¼æƒ…å ±ã®å–å¾—
        price_results = serp_system.get_parts_price_info(query)
        
        if price_results and 'results' in price_results:
            return jsonify({
                'success': True,
                'results': price_results['results'],
                'query_info': price_results.get('query_info', {}),
                'intent_analysis': price_results.get('intent_analysis', {})
            })
        else:
            return jsonify({
                'success': True,
                'results': [],
                'message': 'ä¾¡æ ¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚'
            })
            
    except Exception as e:
        logger.error(f"éƒ¨å“ä¾¡æ ¼APIã‚¨ãƒ©ãƒ¼: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ä¾¡æ ¼æƒ…å ±å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯API"""
    return jsonify({
        'status': 'healthy',
        'message': 'ä¿®ç†å°‚é–€ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚»ãƒ³ã‚¿ãƒ¼ API ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚',
        'features': {
            'rag_system': RAG_AVAILABLE and rag_db is not None,
            'notion_integration': NOTION_AVAILABLE and notion_client is not None,
            'serp_search': SERP_AVAILABLE and serp_system is not None
        }
    })

if __name__ == '__main__':
    print("ğŸ”§ ä¿®ç†å°‚é–€ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚»ãƒ³ã‚¿ãƒ¼ API ã‚’èµ·å‹•ä¸­...")
    print("ğŸ“± ã‚¢ã‚¯ã‚»ã‚¹URL: http://localhost:5000")
    print("ğŸ” API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: http://localhost:5000/api/search")
    print("ğŸ’š ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯: http://localhost:5000/api/health")
    
    app.run(debug=True, host='0.0.0.0', port=5000)
