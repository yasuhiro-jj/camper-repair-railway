#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ•ã‚§ãƒ¼ã‚º2-3: è¨ºæ–­ãƒ•ãƒ­ãƒ¼ã®åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¨ºæ–­ãƒ•ãƒ­ãƒ¼ã‚’åˆ†æã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚„æ”¹å–„ç‚¹ã‚’ç‰¹å®šã™ã‚‹
"""

import os
import json
from typing import Dict, List, Any
from datetime import datetime
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

try:
    from data_access.notion_client import notion_client
    NOTION_AVAILABLE = True
    print("âœ… Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
except ImportError as e:
    NOTION_AVAILABLE = False
    print(f"âš ï¸ Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")


def load_diagnostic_data() -> Dict[str, Any]:
    """è¨ºæ–­ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    if not NOTION_AVAILABLE:
        print("âš ï¸ Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return {"nodes": []}
    
    try:
        # Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç›´æ¥ä½¿ç”¨
        print("ğŸ“¥ Notionã‹ã‚‰è¨ºæ–­ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        diagnostic_data = notion_client.load_diagnostic_data()
        print(f"âœ… {len(diagnostic_data.get('nodes', []))}ä»¶ã®ãƒãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¾ã—ãŸ")
        return diagnostic_data if diagnostic_data else {"nodes": []}
    except Exception as e:
        print(f"âš ï¸ è¨ºæ–­ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return {"nodes": []}


def analyze_diagnostic_flow(diagnostic_data: Dict[str, Any]) -> Dict[str, Any]:
    """è¨ºæ–­ãƒ•ãƒ­ãƒ¼ã‚’åˆ†æã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š"""
    
    nodes = diagnostic_data.get("nodes", [])
    
    if not nodes:
        return {
            "error": "è¨ºæ–­ãƒãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
            "total_nodes": 0
        }
    
    # çµ±è¨ˆæƒ…å ±ã‚’åé›†
    stats = {
        "total_nodes": len(nodes),
        "categories": {},
        "urgency_levels": {},
        "choices_distribution": [],
        "question_lengths": [],
        "has_routing_config": 0,
        "no_routing_config": 0,
        "avg_choices": 0,
        "avg_question_length": 0,
        "max_question_length": 0,
        "min_question_length": float('inf')
    }
    
    total_choices = 0
    
    for node in nodes:
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é›†è¨ˆ
        category = node.get("category", "ä¸æ˜")
        stats["categories"][category] = stats["categories"].get(category, 0) + 1
        
        # ç·Šæ€¥åº¦ã®é›†è¨ˆ
        urgency = node.get("urgency", "ä¸æ˜")
        stats["urgency_levels"][urgency] = stats["urgency_levels"].get(urgency, 0) + 1
        
        # è³ªå•æ–‡ã®é•·ã•
        question = node.get("question", "")
        question_length = len(question)
        stats["question_lengths"].append(question_length)
        stats["max_question_length"] = max(stats["max_question_length"], question_length)
        stats["min_question_length"] = min(stats["min_question_length"], question_length)
        
        # é¸æŠè‚¢ã®æ•°
        routing_config = node.get("routing_config", {})
        if routing_config and "next_nodes_map" in routing_config:
            stats["has_routing_config"] += 1
            next_nodes = routing_config.get("next_nodes_map", [])
            choices_count = len(next_nodes)
            stats["choices_distribution"].append(choices_count)
            total_choices += choices_count
        else:
            stats["no_routing_config"] += 1
    
    # å¹³å‡å€¤ã®è¨ˆç®—
    if nodes:
        if stats["choices_distribution"]:
            stats["avg_choices"] = sum(stats["choices_distribution"]) / len(stats["choices_distribution"])
        stats["avg_question_length"] = sum(stats["question_lengths"]) / len(stats["question_lengths"])
    
    return stats


def find_bottlenecks(diagnostic_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š"""
    
    bottlenecks = []
    nodes = diagnostic_data.get("nodes", [])
    
    for node in nodes:
        node_id = node.get("id", "ä¸æ˜")
        question = node.get("question", "")
        
        # 1. é•·ã™ãã‚‹è³ªå•æ–‡ï¼ˆ100æ–‡å­—ä»¥ä¸Šï¼‰
        if len(question) > 100:
            bottlenecks.append({
                "node_id": node_id,
                "type": "é•·ã™ãã‚‹è³ªå•",
                "severity": "ä¸­",
                "detail": f"è³ªå•æ–‡ãŒ{len(question)}æ–‡å­—ã¨é•·ã™ãã¾ã™ï¼ˆæ¨å¥¨: 50æ–‡å­—ä»¥å†…ï¼‰",
                "question_preview": question[:50] + "..."
            })
        
        # 2. é¸æŠè‚¢ãŒå¤šã™ãã‚‹ãƒãƒ¼ãƒ‰ï¼ˆ5æŠä»¥ä¸Šï¼‰
        routing_config = node.get("routing_config", {})
        if routing_config and "next_nodes_map" in routing_config:
            next_nodes = routing_config.get("next_nodes_map", [])
            if len(next_nodes) >= 5:
                bottlenecks.append({
                    "node_id": node_id,
                    "type": "é¸æŠè‚¢ãŒå¤šã™ãã‚‹",
                    "severity": "é«˜",
                    "detail": f"{len(next_nodes)}å€‹ã®é¸æŠè‚¢ãŒã‚ã‚Šã¾ã™ï¼ˆæ¨å¥¨: 3-4å€‹ä»¥å†…ï¼‰",
                    "choices_count": len(next_nodes)
                })
        
        # 3. ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨­å®šãŒãªã„ãƒãƒ¼ãƒ‰
        if not routing_config or "next_nodes_map" not in routing_config:
            # è¨ºæ–­çµæœãƒãƒ¼ãƒ‰ã§ãªã‘ã‚Œã°å•é¡Œ
            if not node.get("diagnosis_result"):
                bottlenecks.append({
                    "node_id": node_id,
                    "type": "ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨­å®šãªã—",
                    "severity": "é«˜",
                    "detail": "æ¬¡ã®ãƒãƒ¼ãƒ‰ã¸ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨­å®šãŒã‚ã‚Šã¾ã›ã‚“",
                    "question_preview": question[:50] + "..."
                })
        
        # 4. å°‚é–€ç”¨èªãŒå¤šã„ãƒãƒ¼ãƒ‰
        technical_terms = [
            "ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼", "ã‚ªãƒ«ã‚¿ãƒãƒ¼ã‚¿ãƒ¼", "ã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼", "ã‚½ãƒ¬ãƒã‚¤ãƒ‰",
            "ãƒãƒ«ãƒ–", "ãƒ¬ã‚®ãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼", "ã‚³ãƒ³ãƒ‡ãƒ³ã‚µãƒ¼", "ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«", "ã‚»ãƒ³ã‚µãƒ¼"
        ]
        found_terms = [term for term in technical_terms if term in question]
        if len(found_terms) >= 2:
            bottlenecks.append({
                "node_id": node_id,
                "type": "å°‚é–€ç”¨èªãŒå¤šã„",
                "severity": "ä¸­",
                "detail": f"å°‚é–€ç”¨èªãŒ{len(found_terms)}å€‹å«ã¾ã‚Œã¦ã„ã¾ã™: {', '.join(found_terms)}",
                "question_preview": question[:50] + "..."
            })
    
    return bottlenecks


def identify_redundant_nodes(diagnostic_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ä¸è¦ãªãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š"""
    
    redundant = []
    nodes = diagnostic_data.get("nodes", [])
    
    for i, node in enumerate(nodes):
        node_id = node.get("id", "ä¸æ˜")
        question = node.get("question", "")
        routing_config = node.get("routing_config", {})
        
        # 1. æ¬¡ã®ãƒãƒ¼ãƒ‰ãŒ1ã¤ã—ã‹ãªã„ï¼ˆåˆ†å²ãŒãªã„ï¼‰
        if routing_config and "next_nodes_map" in routing_config:
            next_nodes = routing_config.get("next_nodes_map", [])
            if len(next_nodes) == 1:
                redundant.append({
                    "node_id": node_id,
                    "reason": "åˆ†å²ãªã—ï¼ˆçµ±åˆå¯èƒ½ï¼‰",
                    "recommendation": "å‰å¾Œã®ãƒãƒ¼ãƒ‰ã¨çµ±åˆã‚’æ¤œè¨",
                    "question_preview": question[:50] + "..."
                })
        
        # 2. è³ªå•ãŒé¡ä¼¼ã—ã¦ã„ã‚‹ï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰
        for j, other_node in enumerate(nodes):
            if i < j:  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’é¿ã‘ã‚‹
                other_question = other_node.get("question", "")
                # ç°¡æ˜“çš„ãªé¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆå…±é€šã™ã‚‹æ–‡å­—åˆ—ã®å‰²åˆï¼‰
                if question and other_question:
                    similarity = calculate_simple_similarity(question, other_question)
                    if similarity > 0.7:
                        redundant.append({
                            "node_id": node_id,
                            "reason": f"è³ªå•ãŒé¡ä¼¼ï¼ˆé¡ä¼¼åº¦: {similarity:.2f}ï¼‰",
                            "similar_to": other_node.get("id", "ä¸æ˜"),
                            "recommendation": "è³ªå•ã‚’çµ±åˆã¾ãŸã¯å·®åˆ¥åŒ–ã‚’æ¤œè¨",
                            "question_preview": question[:50] + "...",
                            "similar_question_preview": other_question[:50] + "..."
                        })
    
    return redundant


def calculate_simple_similarity(text1: str, text2: str) -> float:
    """ç°¡æ˜“çš„ãªé¡ä¼¼åº¦è¨ˆç®—ï¼ˆå…±é€šã™ã‚‹å˜èªã®å‰²åˆï¼‰"""
    # æ–‡å­—ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“è¨ˆç®—
    words1 = set(text1.split())
    words2 = set(text2.split())
    
    if not words1 or not words2:
        return 0.0
    
    common_words = words1.intersection(words2)
    total_words = len(words1.union(words2))
    
    return len(common_words) / total_words if total_words > 0 else 0.0


def calculate_flow_depth(diagnostic_data: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒ•ãƒ­ãƒ¼ã®æ·±ã•ï¼ˆæœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼‰ã‚’è¨ˆç®—"""
    nodes = diagnostic_data.get("nodes", [])
    
    # ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’æ§‹ç¯‰
    graph = {}
    start_nodes = []
    end_nodes = []
    
    for node in nodes:
        node_id = node.get("id")
        routing_config = node.get("routing_config", {})
        
        # é–‹å§‹ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®šï¼ˆcategoryãŒ"é–‹å§‹"ã®ãƒãƒ¼ãƒ‰ï¼‰
        if node.get("category") == "é–‹å§‹" or not routing_config:
            if node.get("diagnosis_result"):  # è¨ºæ–­çµæœãŒã‚ã‚Œã°çµ‚äº†ãƒãƒ¼ãƒ‰
                end_nodes.append(node_id)
            else:
                start_nodes.append(node_id)
        
        # éš£æ¥ãƒãƒ¼ãƒ‰ã‚’è¨˜éŒ²
        if routing_config and "next_nodes_map" in routing_config:
            next_nodes = routing_config.get("next_nodes_map", [])
            graph[node_id] = [n.get("id") for n in next_nodes if n.get("id")]
        else:
            graph[node_id] = []
            if node.get("diagnosis_result"):
                end_nodes.append(node_id)
    
    # BFSã§æœ€å¤§æ·±ã•ã‚’è¨ˆç®—
    max_depth = 0
    paths = []
    
    for start_node in start_nodes:
        depth = bfs_max_depth(graph, start_node, end_nodes)
        if depth > max_depth:
            max_depth = depth
        paths.append({
            "start_node": start_node,
            "max_depth": depth
        })
    
    return {
        "max_depth": max_depth,
        "start_nodes_count": len(start_nodes),
        "end_nodes_count": len(end_nodes),
        "paths": paths
    }


def bfs_max_depth(graph: Dict[str, List[str]], start_node: str, end_nodes: List[str]) -> int:
    """BFSã‚’ä½¿ã£ã¦é–‹å§‹ãƒãƒ¼ãƒ‰ã‹ã‚‰çµ‚äº†ãƒãƒ¼ãƒ‰ã¾ã§ã®æœ€å¤§æ·±ã•ã‚’è¨ˆç®—"""
    from collections import deque
    
    if start_node in end_nodes:
        return 0
    
    visited = set()
    queue = deque([(start_node, 0)])
    max_depth = 0
    
    while queue:
        current_node, depth = queue.popleft()
        
        if current_node in visited:
            continue
        
        visited.add(current_node)
        
        if current_node in end_nodes:
            max_depth = max(max_depth, depth)
            continue
        
        # æ¬¡ã®ãƒãƒ¼ãƒ‰ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        for next_node in graph.get(current_node, []):
            if next_node not in visited:
                queue.append((next_node, depth + 1))
    
    return max_depth


def generate_analysis_report(diagnostic_data: Dict[str, Any]) -> str:
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    print("\n" + "=" * 80)
    print("ğŸ” è¨ºæ–­ãƒ•ãƒ­ãƒ¼åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 80)
    print(f"ğŸ“… åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # çµ±è¨ˆæƒ…å ±
    stats = analyze_diagnostic_flow(diagnostic_data)
    print("ğŸ“Š åŸºæœ¬çµ±è¨ˆ")
    print("-" * 80)
    print(f"ç·ãƒãƒ¼ãƒ‰æ•°: {stats['total_nodes']}")
    print(f"ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨­å®šã‚ã‚Š: {stats['has_routing_config']}")
    print(f"ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨­å®šãªã—: {stats['no_routing_config']}")
    print(f"å¹³å‡é¸æŠè‚¢æ•°: {stats['avg_choices']:.2f}")
    print(f"å¹³å‡è³ªå•æ–‡é•·: {stats['avg_question_length']:.1f}æ–‡å­—")
    print(f"æœ€é•·è³ªå•æ–‡: {stats['max_question_length']}æ–‡å­—")
    print(f"æœ€çŸ­è³ªå•æ–‡: {stats['min_question_length']}æ–‡å­—\n")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é›†è¨ˆ
    print("ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒãƒ¼ãƒ‰æ•°")
    print("-" * 80)
    for category, count in sorted(stats['categories'].items(), key=lambda x: x[1], reverse=True):
        print(f"  {category}: {count}ä»¶")
    print()
    
    # ç·Šæ€¥åº¦åˆ¥ã®é›†è¨ˆ
    print("âš ï¸ ç·Šæ€¥åº¦åˆ¥ãƒãƒ¼ãƒ‰æ•°")
    print("-" * 80)
    for urgency, count in sorted(stats['urgency_levels'].items(), key=lambda x: x[1], reverse=True):
        print(f"  {urgency}: {count}ä»¶")
    print()
    
    # ãƒ•ãƒ­ãƒ¼ã®æ·±ã•
    depth_info = calculate_flow_depth(diagnostic_data)
    print("ğŸ”¢ ãƒ•ãƒ­ãƒ¼ã®æ·±ã•")
    print("-" * 80)
    print(f"æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°: {depth_info['max_depth']}")
    print(f"é–‹å§‹ãƒãƒ¼ãƒ‰æ•°: {depth_info['start_nodes_count']}")
    print(f"çµ‚äº†ãƒãƒ¼ãƒ‰æ•°: {depth_info['end_nodes_count']}")
    print()
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
    bottlenecks = find_bottlenecks(diagnostic_data)
    print(f"âš ï¸ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ ({len(bottlenecks)}ä»¶)")
    print("-" * 80)
    if bottlenecks:
        # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        severity_order = {"é«˜": 0, "ä¸­": 1, "ä½": 2}
        bottlenecks_sorted = sorted(bottlenecks, key=lambda x: severity_order.get(x.get("severity", "ä½"), 3))
        
        for i, bottleneck in enumerate(bottlenecks_sorted[:10], 1):  # ä¸Šä½10ä»¶
            print(f"{i}. [{bottleneck['severity']}] {bottleneck['type']}")
            print(f"   ãƒãƒ¼ãƒ‰ID: {bottleneck['node_id']}")
            print(f"   è©³ç´°: {bottleneck['detail']}")
            if 'question_preview' in bottleneck:
                print(f"   è³ªå•: {bottleneck['question_preview']}")
            print()
    else:
        print("ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    print()
    
    # ä¸è¦ãªãƒãƒ¼ãƒ‰
    redundant = identify_redundant_nodes(diagnostic_data)
    print(f"ğŸ”„ ä¸è¦ãªãƒãƒ¼ãƒ‰å€™è£œ ({len(redundant)}ä»¶)")
    print("-" * 80)
    if redundant:
        for i, item in enumerate(redundant[:5], 1):  # ä¸Šä½5ä»¶
            print(f"{i}. ãƒãƒ¼ãƒ‰ID: {item['node_id']}")
            print(f"   ç†ç”±: {item['reason']}")
            print(f"   æ¨å¥¨: {item['recommendation']}")
            if 'question_preview' in item:
                print(f"   è³ªå•: {item['question_preview']}")
            print()
    else:
        print("ä¸è¦ãªãƒãƒ¼ãƒ‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    print()
    
    # æ”¹å–„ææ¡ˆ
    print("ğŸ’¡ æ”¹å–„ææ¡ˆ")
    print("-" * 80)
    
    suggestions = []
    
    if stats['avg_question_length'] > 60:
        suggestions.append("â€¢ è³ªå•æ–‡ã‚’çŸ­ãã™ã‚‹ã“ã¨ã‚’æ¤œè¨ï¼ˆç›®æ¨™: 50æ–‡å­—ä»¥å†…ï¼‰")
    
    if stats['avg_choices'] > 4:
        suggestions.append(f"â€¢ é¸æŠè‚¢ã‚’æ¸›ã‚‰ã™ã“ã¨ã‚’æ¤œè¨ï¼ˆç¾åœ¨å¹³å‡: {stats['avg_choices']:.1f}å€‹ã€ç›®æ¨™: 3-4å€‹ï¼‰")
    
    if depth_info['max_depth'] > 5:
        suggestions.append(f"â€¢ ãƒ•ãƒ­ãƒ¼ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å‰Šæ¸›ï¼ˆç¾åœ¨: {depth_info['max_depth']}ã‚¹ãƒ†ãƒƒãƒ—ã€ç›®æ¨™: 3-5ã‚¹ãƒ†ãƒƒãƒ—ï¼‰")
    
    if len([b for b in bottlenecks if b.get('type') == 'å°‚é–€ç”¨èªãŒå¤šã„']) > 0:
        suggestions.append("â€¢ å°‚é–€ç”¨èªã‚’å¹³æ˜“ãªè¨€è‘‰ã«ç½®ãæ›ãˆã‚‹")
    
    if len(redundant) > 0:
        suggestions.append(f"â€¢ {len(redundant)}å€‹ã®ä¸è¦ãªãƒãƒ¼ãƒ‰å€™è£œã‚’çµ±åˆã¾ãŸã¯å‰Šé™¤")
    
    if not suggestions:
        suggestions.append("â€¢ ç‰¹ã«å¤§ããªå•é¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼")
    
    for suggestion in suggestions:
        print(suggestion)
    
    print("\n" + "=" * 80)
    print("åˆ†æå®Œäº† âœ…")
    print("=" * 80 + "\n")
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONã§ä¿å­˜
    report = {
        "timestamp": datetime.now().isoformat(),
        "stats": stats,
        "depth_info": depth_info,
        "bottlenecks": bottlenecks,
        "redundant_nodes": redundant,
        "suggestions": suggestions
    }
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    report_filename = f"diagnostic_flow_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_filename, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {report_filename}")
    
    return report


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ è¨ºæ–­ãƒ•ãƒ­ãƒ¼åˆ†æã‚’é–‹å§‹ã—ã¾ã™...\n")
    
    # è¨ºæ–­ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    diagnostic_data = load_diagnostic_data()
    
    if not diagnostic_data or not diagnostic_data.get("nodes"):
        print("âŒ è¨ºæ–­ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    report = generate_analysis_report(diagnostic_data)
    
    print("\nâœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")


if __name__ == "__main__":
    main()

