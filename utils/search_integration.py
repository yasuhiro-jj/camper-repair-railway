"""
çµ±åˆæ¤œç´¢æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

è¤‡æ•°ã®æ¤œç´¢ã‚½ãƒ¼ã‚¹ï¼ˆRAGã€SERPã€Notionï¼‰ã®çµæœã‚’çµ±åˆã—ã€
å‹•çš„ãªé‡ã¿ä»˜ã‘ã€ãƒãƒ¼ã‚¸ã€é‡è¤‡æ’é™¤ã‚’è¡Œã†
"""

import time
from typing import List, Dict, Any, Optional
from difflib import SequenceMatcher


class SearchIntegration:
    """çµ±åˆæ¤œç´¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é‡ã¿ä»˜ã‘
        self.default_weights = {
            'notion': 1.0,   # æœ€å„ªå…ˆï¼ˆæœ€ã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰
            'rag': 0.8,      # è£œå®Œï¼ˆæŠ€è¡“çš„ã«è©³ç´°ï¼‰
            'serp': 0.6      # å‚è€ƒï¼ˆæœ€æ–°æƒ…å ±ãƒ»ä¾¡æ ¼æƒ…å ±ï¼‰
        }
    
    def calculate_dynamic_weights(
        self,
        query: str,
        intent: Dict[str, Any]
    ) -> Dict[str, float]:
        """
        ã‚¯ã‚¨ãƒªã¨æ„å›³ã«åŸºã¥ã„ã¦å‹•çš„ã«é‡ã¿ã‚’è¨ˆç®—
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            intent: æ„å›³æƒ…å ±
        
        Returns:
            å„ã‚½ãƒ¼ã‚¹ã®é‡ã¿
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼
        weights = self.default_weights.copy()
        
        # ã‚¯ã‚¨ãƒªã®ç¨®é¡ã‚’ç‰¹å®š
        query_lower = query.lower()
        
        # 1. ä¾¡æ ¼æƒ…å ±ã‚¯ã‚¨ãƒª
        price_keywords = ['ä¾¡æ ¼', 'å€¤æ®µ', 'è²»ç”¨', 'ã„ãã‚‰', 'ã‚³ã‚¹ãƒˆ', 'æ–™é‡‘', 'ç›¸å ´']
        if any(keyword in query_lower for keyword in price_keywords):
            weights['serp'] = 1.0      # SERPæœ€å„ªå…ˆ
            weights['notion'] = 0.7    # Notionã¯è£œå®Œ
            weights['rag'] = 0.5       # RAGã¯å‚è€ƒ
            print("ğŸ’° ä¾¡æ ¼æƒ…å ±ã‚¯ã‚¨ãƒª: SERPé‡è¦–")
        
        # 2. æœ€æ–°æƒ…å ±ã‚¯ã‚¨ãƒª
        elif any(keyword in query_lower for keyword in ['æœ€æ–°', 'æ–°ã—ã„', 'æœ€è¿‘', 'ä»Š', 'ç¾åœ¨', '2024', '2025']):
            weights['serp'] = 1.0      # SERPæœ€å„ªå…ˆ
            weights['notion'] = 0.8    # Notionã‚‚é‡è¦
            weights['rag'] = 0.5       # RAGã¯å¤ã„å¯èƒ½æ€§
            print("ğŸ†• æœ€æ–°æƒ…å ±ã‚¯ã‚¨ãƒª: SERPé‡è¦–")
        
        # 3. ç·Šæ€¥åº¦ãŒé«˜ã„ã‚¯ã‚¨ãƒª
        elif intent.get('urgency') == 'high':
            weights['notion'] = 1.0    # Notionæœ€å„ªå…ˆï¼ˆä¿¡é ¼æ€§é‡è¦–ï¼‰
            weights['rag'] = 0.6       # RAGã¯è£œå®Œ
            weights['serp'] = 0.4      # SERPã¯å‚è€ƒç¨‹åº¦
            print("ğŸš¨ ç·Šæ€¥ã‚¯ã‚¨ãƒª: Notioné‡è¦–ï¼ˆä¿¡é ¼æ€§å„ªå…ˆï¼‰")
        
        # 4. ä¿®ç†ãƒ»è¨ºæ–­ã‚¯ã‚¨ãƒª
        elif any(keyword in query_lower for keyword in ['ä¿®ç†', 'ç›´ã™', 'å¯¾å‡¦', 'è§£æ±º', 'æ–¹æ³•', 'æ‰‹é †', 'è¨ºæ–­', 'åŸå› ', 'ç—‡çŠ¶', 'ãƒã‚§ãƒƒã‚¯', 'ç¢ºèª']):
            weights['rag'] = 1.0       # RAGæœ€å„ªå…ˆï¼ˆæŠ€è¡“æƒ…å ±è±Šå¯Œï¼‰
            weights['notion'] = 0.9    # Notionã‚‚é‡è¦
            weights['serp'] = 0.5      # SERPã¯è£œå®Œ
            print("ğŸ”§ ä¿®ç†ãƒ»è¨ºæ–­ã‚¯ã‚¨ãƒª: RAGé‡è¦–ï¼ˆæŠ€è¡“æƒ…å ±å„ªå…ˆï¼‰")
        
        # 5. æ¥­è€…ãƒ»å·¥å ´æ¤œç´¢ã‚¯ã‚¨ãƒª
        elif any(keyword in query_lower for keyword in ['æ¥­è€…', 'å·¥å ´', 'åº—èˆ—', 'ã‚·ãƒ§ãƒƒãƒ—', 'ä¿®ç†åº—', 'ã©ã“', 'è¿‘ã']):
            weights['notion'] = 1.0    # Notionæœ€å„ªå…ˆï¼ˆDBæƒ…å ±ï¼‰
            weights['serp'] = 0.8      # SERPã‚‚é‡è¦
            weights['rag'] = 0.4       # RAGã¯å‚è€ƒ
            print("ğŸ­ æ¥­è€…æ¤œç´¢ã‚¯ã‚¨ãƒª: Notioné‡è¦–ï¼ˆDBæƒ…å ±å„ªå…ˆï¼‰")
        
        # 6. ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»è©•åˆ¤ã‚¯ã‚¨ãƒª
        elif any(keyword in query_lower for keyword in ['ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'è©•åˆ¤', 'å£ã‚³ãƒŸ', 'ãŠã™ã™ã‚', 'æ¯”è¼ƒ']):
            weights['serp'] = 1.0      # SERPæœ€å„ªå…ˆ
            weights['notion'] = 0.7    # Notionã‚‚å‚è€ƒ
            weights['rag'] = 0.5       # RAGã¯å‚è€ƒ
            print("â­ ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¯ã‚¨ãƒª: SERPé‡è¦–")
        
        # 7. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆä¸€èˆ¬çš„ãªã‚¯ã‚¨ãƒªï¼‰
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é‡ã¿ã‚’ä½¿ç”¨
            print("ğŸ“ ä¸€èˆ¬ã‚¯ã‚¨ãƒª: ãƒãƒ©ãƒ³ã‚¹å‹")
        
        return weights
    
    def calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        
        Args:
            text1: ãƒ†ã‚­ã‚¹ãƒˆ1
            text2: ãƒ†ã‚­ã‚¹ãƒˆ2
        
        Returns:
            é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0.0ã€œ1.0ï¼‰
        """
        # ç©ºæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
        if not text1 or not text2:
            return 0.0
        
        # SequenceMatcherã§é¡ä¼¼åº¦ã‚’è¨ˆç®—
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    
    def deduplicate_by_similarity(
        self,
        results: List[Dict[str, Any]],
        similarity_threshold: float = 0.85
    ) -> List[Dict[str, Any]]:
        """
        é¡ä¼¼åº¦ãŒé«˜ã„çµæœã‚’é‡è¤‡æ’é™¤
        
        Args:
            results: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
            similarity_threshold: é¡ä¼¼åº¦ã®é–¾å€¤
        
        Returns:
            é‡è¤‡æ’é™¤ã•ã‚ŒãŸçµæœ
        """
        if not results:
            return []
        
        unique_results = []
        
        for result in results:
            is_duplicate = False
            result_content = result.get('content', '')
            
            # æ—¢å­˜ã®çµæœã¨æ¯”è¼ƒ
            for existing in unique_results:
                existing_content = existing.get('content', '')
                
                # é¡ä¼¼åº¦ã‚’è¨ˆç®—
                similarity = self.calculate_text_similarity(
                    result_content,
                    existing_content
                )
                
                # é–¾å€¤ä»¥ä¸Šãªã‚‰é‡è¤‡ã¨åˆ¤å®š
                if similarity >= similarity_threshold:
                    is_duplicate = True
                    
                    # ã‚¹ã‚³ã‚¢ãŒé«˜ã„æ–¹ã‚’æ®‹ã™
                    if result.get('total_score', 0) > existing.get('total_score', 0):
                        # æ—¢å­˜ã®çµæœã‚’å‰Šé™¤ã—ã¦æ–°ã—ã„çµæœã‚’è¿½åŠ 
                        unique_results.remove(existing)
                        unique_results.append(result)
                    
                    break
            
            # é‡è¤‡ã§ãªã‘ã‚Œã°è¿½åŠ 
            if not is_duplicate:
                unique_results.append(result)
        
        return unique_results
    
    def deduplicate_by_url(
        self,
        results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        URLãƒ™ãƒ¼ã‚¹ã§é‡è¤‡æ’é™¤ï¼ˆé«˜é€Ÿï¼‰
        
        Args:
            results: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        
        Returns:
            URLé‡è¤‡æ’é™¤ã•ã‚ŒãŸçµæœ
        """
        if not results:
            return []
        
        seen_urls = set()
        unique_results = []
        
        for result in results:
            url = result.get('url', '')
            
            # URLãŒå­˜åœ¨ã—ã€ã¾ã è¦‹ã¦ã„ãªã„å ´åˆã¯è¿½åŠ 
            if url:
                # URLã‚’æ­£è¦åŒ–ï¼ˆã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»ï¼‰
                normalized_url = url.split('?')[0].split('#')[0].rstrip('/')
                
                if normalized_url not in seen_urls:
                    seen_urls.add(normalized_url)
                    unique_results.append(result)
                else:
                    # é‡è¤‡URLã®å ´åˆã€ã‚¹ã‚³ã‚¢ãŒé«˜ã„æ–¹ã‚’æ®‹ã™
                    for i, existing in enumerate(unique_results):
                        existing_url = existing.get('url', '')
                        existing_normalized = existing_url.split('?')[0].split('#')[0].rstrip('/')
                        
                        if existing_normalized == normalized_url:
                            # ã‚¹ã‚³ã‚¢ãŒé«˜ã„æ–¹ã‚’æ®‹ã™
                            if result.get('weighted_score', 0) > existing.get('weighted_score', 0):
                                unique_results[i] = result
                            break
            else:
                # URLãŒãªã„å ´åˆã¯ãã®ã¾ã¾è¿½åŠ ï¼ˆå¾Œã§é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã§å‡¦ç†ï¼‰
                unique_results.append(result)
        
        return unique_results
    
    def merge_search_results(
        self,
        rag_results: Dict[str, Any],
        serp_results: Dict[str, Any],
        notion_results: Dict[str, Any],
        weights: Dict[str, float],
        max_results: int = 10
    ) -> List[Dict[str, Any]]:
        """
        è¤‡æ•°ã®æ¤œç´¢çµæœã‚’ãƒãƒ¼ã‚¸
        
        Args:
            rag_results: RAGæ¤œç´¢çµæœ
            serp_results: SERPæ¤œç´¢çµæœ
            notion_results: Notionæ¤œç´¢çµæœ
            weights: å„ã‚½ãƒ¼ã‚¹ã®é‡ã¿
            max_results: æœ€å¤§çµæœæ•°
        
        Returns:
            ãƒãƒ¼ã‚¸ã•ã‚ŒãŸçµæœã®ãƒªã‚¹ãƒˆ
        """
        merged = []
        
        # 1. RAGæ¤œç´¢çµæœã‚’è¿½åŠ 
        if rag_results and 'search_results' in rag_results:
            for result in rag_results['search_results']:
                merged.append({
                    'source': 'rag',
                    'title': result.get('title', 'æ¤œç´¢çµæœ'),
                    'content': result.get('content', ''),
                    'url': result.get('url', ''),
                    'category': result.get('category', 'ä¸æ˜'),
                    'base_score': result.get('relevance_score', 0.7),
                    'weighted_score': result.get('relevance_score', 0.7) * weights['rag'],
                    'metadata': result.get('metadata', {})
                })
        
        # RAGçµæœãŒresultsã‚­ãƒ¼ã«ã‚ã‚‹å ´åˆï¼ˆå¼·åŒ–ç‰ˆï¼‰
        elif rag_results and 'results' in rag_results:
            for result in rag_results['results']:
                merged.append({
                    'source': 'rag',
                    'title': result.get('title', 'æ¤œç´¢çµæœ'),
                    'content': result.get('content', ''),
                    'url': result.get('url', ''),
                    'category': result.get('category', 'ä¸æ˜'),
                    'base_score': result.get('relevance_score', 0.7),
                    'weighted_score': result.get('relevance_score', 0.7) * weights['rag'],
                    'metadata': result.get('metadata', {})
                })
        
        # 2. SERPæ¤œç´¢çµæœã‚’è¿½åŠ 
        if serp_results and 'results' in serp_results:
            for result in serp_results['results']:
                merged.append({
                    'source': 'serp',
                    'title': result.get('title', 'æ¤œç´¢çµæœ'),
                    'content': result.get('snippet', result.get('content', '')),
                    'url': result.get('url', ''),
                    'category': 'SERPæ¤œç´¢',
                    'base_score': result.get('total_score', result.get('relevance', 0.6)),
                    'weighted_score': result.get('total_score', result.get('relevance', 0.6)) * weights['serp'],
                    'metadata': {
                        'trust_score': result.get('trust_score', 0.5),
                        'relevance_score': result.get('relevance_score', 0.5)
                    }
                })
        
        # 3. Notionæ¤œç´¢çµæœã‚’è¿½åŠ 
        if notion_results:
            # ä¿®ç†ã‚±ãƒ¼ã‚¹
            for case in notion_results.get('repair_cases', []):
                merged.append({
                    'source': 'notion',
                    'title': case.get('title', 'ä¿®ç†ã‚±ãƒ¼ã‚¹'),
                    'content': case.get('solution', case.get('content', '')),
                    'url': case.get('url', ''),
                    'category': case.get('category', 'ä¿®ç†ã‚±ãƒ¼ã‚¹'),
                    'base_score': case.get('relevance_score', 0.8),
                    'weighted_score': case.get('relevance_score', 0.8) * weights['notion'],
                    'metadata': {
                        'matched_keywords': case.get('matched_keywords', []),
                        'database': 'repair_cases'
                    }
                })
            
            # è¨ºæ–­ãƒãƒ¼ãƒ‰
            for node in notion_results.get('diagnostic_nodes', []):
                merged.append({
                    'source': 'notion',
                    'title': node.get('title', 'è¨ºæ–­ãƒãƒ¼ãƒ‰'),
                    'content': node.get('diagnosis_result', node.get('question', '')),
                    'url': node.get('url', ''),
                    'category': node.get('category', 'è¨ºæ–­'),
                    'base_score': node.get('relevance_score', 0.8),
                    'weighted_score': node.get('relevance_score', 0.8) * weights['notion'],
                    'metadata': {
                        'matched_keywords': node.get('matched_keywords', []),
                        'database': 'diagnostic_nodes'
                    }
                })
        
        # 4. é‡è¤‡æ’é™¤ï¼ˆURLãƒ™ãƒ¼ã‚¹ â†’ é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
        print(f"ğŸ“Š ãƒãƒ¼ã‚¸å‰: {len(merged)}ä»¶")
        # ã¾ãšURLãƒ™ãƒ¼ã‚¹ã§é‡è¤‡æ’é™¤ï¼ˆé«˜é€Ÿï¼‰
        unique_results = self.deduplicate_by_url(merged)
        print(f"ğŸ“Š URLé‡è¤‡æ’é™¤å¾Œ: {len(unique_results)}ä»¶")
        # æ¬¡ã«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã§é‡è¤‡æ’é™¤ï¼ˆç²¾åº¦é‡è¦–ï¼‰
        unique_results = self.deduplicate_by_similarity(unique_results, similarity_threshold=0.85)
        print(f"ğŸ“Š é¡ä¼¼åº¦é‡è¤‡æ’é™¤å¾Œ: {len(unique_results)}ä»¶")
        
        # 5. ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(
            unique_results,
            key=lambda x: x['weighted_score'],
            reverse=True
        )
        
        # 6. ç·åˆã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆæ­£è¦åŒ–ï¼‰
        max_score = sorted_results[0]['weighted_score'] if sorted_results else 1.0
        for result in sorted_results:
            result['total_score'] = result['weighted_score'] / max_score if max_score > 0 else 0
        
        # 7. æœ€å¤§ä»¶æ•°ã«åˆ¶é™
        return sorted_results[:max_results]
    
    def get_source_distribution(self, results: List[Dict]) -> Dict[str, int]:
        """
        ã‚½ãƒ¼ã‚¹åˆ¥ã®çµæœåˆ†å¸ƒã‚’å–å¾—
        
        Args:
            results: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        
        Returns:
            ã‚½ãƒ¼ã‚¹åˆ¥ã®ä»¶æ•°
        """
        distribution = {
            'rag': 0,
            'serp': 0,
            'notion': 0
        }
        
        for result in results:
            source = result.get('source', 'unknown')
            if source in distribution:
                distribution[source] += 1
        
        return distribution


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
search_integration = SearchIntegration()


def integrate_search_results(
    rag_results: Dict,
    serp_results: Dict,
    notion_results: Dict,
    query: str,
    intent: Dict
) -> List[Dict]:
    """
    çµ±åˆæ¤œç´¢ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    
    Args:
        rag_results: RAGæ¤œç´¢çµæœ
        serp_results: SERPæ¤œç´¢çµæœ
        notion_results: Notionæ¤œç´¢çµæœ
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        intent: æ„å›³æƒ…å ±
    
    Returns:
        çµ±åˆã•ã‚ŒãŸæ¤œç´¢çµæœ
    """
    # å‹•çš„ãªé‡ã¿ä»˜ã‘ã‚’è¨ˆç®—
    weights = search_integration.calculate_dynamic_weights(query, intent)
    
    # çµæœã‚’ãƒãƒ¼ã‚¸
    merged = search_integration.merge_search_results(
        rag_results,
        serp_results,
        notion_results,
        weights,
        max_results=10
    )
    
    return merged


if __name__ == "__main__":
    print("=== çµ±åˆæ¤œç´¢æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ===")
    print("âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ")
    
    # ãƒ†ã‚¹ãƒˆ
    integration = SearchIntegration()
    
    test_queries = [
        ("ã‚¨ã‚¢ã‚³ãƒ³ã®ä¿®ç†è²»ç”¨ã¯ã„ãã‚‰ã§ã™ã‹", {'urgency': 'low'}),
        ("ãƒãƒƒãƒ†ãƒªãƒ¼ãŒä¸ŠãŒã£ã¦å‹•ã‹ãªã„ï¼", {'urgency': 'high'}),
        ("FFãƒ’ãƒ¼ã‚¿ãƒ¼ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«", {'urgency': 'low'}),
        ("ä¿®ç†æ¥­è€…ã‚’æ¢ã—ã¦ã„ã¾ã™", {'urgency': 'low'})
    ]
    
    print("\nå‹•çš„ãªé‡ã¿ä»˜ã‘ã®ãƒ†ã‚¹ãƒˆ:")
    for query, intent in test_queries:
        print(f"\nã‚¯ã‚¨ãƒª: {query}")
        weights = integration.calculate_dynamic_weights(query, intent)
        print(f"  é‡ã¿: {weights}")

