"""
å¼·åŒ–ç‰ˆRAGæ¤œç´¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã‚¯ã‚¨ãƒªæ‹¡å¼µã€é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å«ã‚€é«˜ç²¾åº¦RAGæ¤œç´¢
"""

import time
from typing import List, Dict, Any, Optional
from langchain_chroma import Chroma
from langchain_core.documents import Document

# ã‚¯ã‚¨ãƒªæ‹¡å¼µãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from utils.query_expander import query_expander
    QUERY_EXPANDER_AVAILABLE = True
except ImportError:
    QUERY_EXPANDER_AVAILABLE = False
    print("âš ï¸ query_expander ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")


def deduplicate_results(results: List[Dict], similarity_threshold: float = 0.9) -> List[Dict]:
    """
    æ¤œç´¢çµæœã®é‡è¤‡ã‚’æ’é™¤
    
    Args:
        results: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        similarity_threshold: é‡è¤‡åˆ¤å®šã®é¡ä¼¼åº¦é–¾å€¤
    
    Returns:
        é‡è¤‡æ’é™¤ã•ã‚ŒãŸçµæœ
    """
    if not results:
        return []
    
    unique_results = []
    seen_contents = set()
    
    for result in results:
        content = result.get('content', '')
        
        # ç°¡æ˜“çš„ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
        content_hash = hash(content[:200])  # æœ€åˆã®200æ–‡å­—ã§ãƒãƒƒã‚·ãƒ¥åŒ–
        
        if content_hash not in seen_contents:
            seen_contents.add(content_hash)
            unique_results.append(result)
    
    return unique_results


def calculate_relevance_score(query: str, document: Document, base_score: float) -> float:
    """
    é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        document: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        base_score: ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ï¼‰
    
    Returns:
        è¨ˆç®—ã•ã‚ŒãŸé–¢é€£æ€§ã‚¹ã‚³ã‚¢
    """
    score = base_score
    
    # ã‚¯ã‚¨ãƒªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹
    query_keywords = query.lower().split()
    content_lower = document.page_content.lower()
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒœãƒ¼ãƒŠã‚¹
    keyword_matches = sum(1 for kw in query_keywords if kw in content_lower)
    if keyword_matches > 0:
        score += 0.1 * (keyword_matches / len(query_keywords))
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒœãƒ¼ãƒŠã‚¹
    metadata = document.metadata
    
    # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹ãƒœãƒ¼ãƒŠã‚¹
    source_type = metadata.get('source_type', '')
    if source_type == 'notion':
        score += 0.1  # Notionãƒ‡ãƒ¼ã‚¿ã¯ä¿¡é ¼æ€§ãŒé«˜ã„
    elif source_type == 'blog':
        score += 0.05  # ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚‚æœ‰ç”¨
    
    # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒãƒœãƒ¼ãƒŠã‚¹
    if 'category' in metadata:
        category = metadata['category']
        if category.lower() in query.lower():
            score += 0.15
    
    return min(score, 1.0)  # æœ€å¤§1.0ã«åˆ¶é™


def enhanced_rag_retrieve_v2(
    query: str,
    db: Chroma,
    max_results: int = 5,
    relevance_threshold: float = 0.65,
    use_query_expansion: bool = True,
    category: str = None
) -> Dict[str, Any]:
    """
    å¼·åŒ–ç‰ˆRAGæ¤œç´¢
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        db: Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        max_results: æœ€å¤§çµæœæ•°
        relevance_threshold: é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®é–¾å€¤
        use_query_expansion: ã‚¯ã‚¨ãƒªæ‹¡å¼µã‚’ä½¿ç”¨ã™ã‚‹ã‹
        category: ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        æ¤œç´¢çµæœã®è¾æ›¸
    """
    start_time = time.time()
    
    print(f"ğŸ” å¼·åŒ–ç‰ˆRAGæ¤œç´¢é–‹å§‹: ã‚¯ã‚¨ãƒª='{query}'")
    
    all_results = []
    queries_used = [query]  # å…ƒã®ã‚¯ã‚¨ãƒª
    
    try:
        # 1. ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if use_query_expansion and QUERY_EXPANDER_AVAILABLE:
            print("ğŸ“ ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µä¸­...")
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸæ‹¡å¼µ
            expansion_result = query_expander.expand_with_context(query, category)
            
            # æ‹¡å¼µã•ã‚ŒãŸã‚¯ã‚¨ãƒª
            expanded_queries = expansion_result['expanded_queries'][:3]  # æœ€å¤§3ã¤
            queries_used = expanded_queries
            
            # é–¢é€£èªã‚‚æ¤œç´¢ã«å«ã‚ã‚‹
            if expansion_result['related_terms']:
                related_query = f"{query} {' '.join(expansion_result['related_terms'][:2])}"
                queries_used.append(related_query)
            
            print(f"âœ… {len(queries_used)}å€‹ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢: {queries_used}")
        
        # 2. å„ã‚¯ã‚¨ãƒªã§æ¤œç´¢å®Ÿè¡Œ
        for search_query in queries_used:
            try:
                # é¡ä¼¼åº¦æ¤œç´¢ï¼ˆã‚¹ã‚³ã‚¢ä»˜ãï¼‰
                results_with_scores = db.similarity_search_with_relevance_scores(
                    search_query,
                    k=max_results * 2  # ä½™åˆ†ã«å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                )
                
                # çµæœã‚’æ•´å½¢
                for doc, score in results_with_scores:
                    # é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’å†è¨ˆç®—
                    enhanced_score = calculate_relevance_score(query, doc, score)
                    
                    all_results.append({
                        'document': doc,
                        'score': enhanced_score,
                        'original_score': score,
                        'query_used': search_query,
                        'content': doc.page_content,
                        'metadata': doc.metadata
                    })
            
            except Exception as e:
                print(f"âš ï¸ ã‚¯ã‚¨ãƒª '{search_query}' ã®æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # 3. é‡è¤‡æ’é™¤
        print(f"ğŸ“Š é‡è¤‡æ’é™¤å‰: {len(all_results)}ä»¶")
        unique_results = deduplicate_results(all_results)
        print(f"ğŸ“Š é‡è¤‡æ’é™¤å¾Œ: {len(unique_results)}ä»¶")
        
        # 4. é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_results = [
            r for r in unique_results 
            if r['score'] >= relevance_threshold
        ]
        print(f"ğŸ“Š é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered_results)}ä»¶ (é–¾å€¤: {relevance_threshold})")
        
        # 5. ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(
            filtered_results,
            key=lambda x: x['score'],
            reverse=True
        )
        
        # 6. æœ€å¤§ä»¶æ•°ã«åˆ¶é™
        final_results = sorted_results[:max_results]
        
        # 7. çµæœã‚’æ•´å½¢
        formatted_results = []
        for i, result in enumerate(final_results):
            doc = result['document']
            metadata = result['metadata']
            
            formatted_results.append({
                'title': metadata.get('title', f'æ¤œç´¢çµæœ {i+1}'),
                'content': result['content'],
                'source': metadata.get('source', 'RAGæ¤œç´¢'),
                'source_type': metadata.get('source_type', 'unknown'),
                'category': metadata.get('category', 'ä¸æ˜'),
                'url': metadata.get('url', ''),
                'relevance_score': round(result['score'], 3),
                'original_score': round(result['original_score'], 3),
                'query_used': result['query_used']
            })
        
        duration = time.time() - start_time
        
        return {
            'results': formatted_results,
            'total_found': len(all_results),
            'after_deduplication': len(unique_results),
            'after_filtering': len(filtered_results),
            'returned': len(final_results),
            'queries_used': queries_used,
            'duration': round(duration, 2),
            'relevance_threshold': relevance_threshold
        }
    
    except Exception as e:
        print(f"âŒ RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            'results': [],
            'error': str(e),
            'duration': time.time() - start_time
        }


def simple_rag_search(query: str, db: Chroma, max_results: int = 5) -> List[Dict]:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªRAGæ¤œç´¢ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        db: Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        max_results: æœ€å¤§çµæœæ•°
    
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
    """
    try:
        results = db.similarity_search(query, k=max_results)
        
        formatted = []
        for doc in results:
            formatted.append({
                'content': doc.page_content,
                'metadata': doc.metadata,
                'source': doc.metadata.get('source', 'RAGæ¤œç´¢')
            })
        
        return formatted
    
    except Exception as e:
        print(f"âŒ RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []


# ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    print("=== RAGæ¤œç´¢å¼·åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ===")
    print("âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ")
    
    if QUERY_EXPANDER_AVAILABLE:
        print("âœ… ã‚¯ã‚¨ãƒªæ‹¡å¼µæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
    else:
        print("âš ï¸ ã‚¯ã‚¨ãƒªæ‹¡å¼µæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")

