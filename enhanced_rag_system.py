#!/usr/bin/env python3
"""
ãƒ–ãƒ­ã‚°URLã‚‚å«ã‚ãŸæ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import glob
import shutil
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader, TextLoader

# ChromaDBã®å®‰å…¨ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from langchain_chroma import Chroma
    CHROMA_AVAILABLE = True
except ImportError:
    Chroma = None
    CHROMA_AVAILABLE = False


def process_markdown_content(content, filename):
    """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹é€ åŒ–ã—ã¦RAGã§æ´»ç”¨ã—ã‚„ã™ãã™ã‚‹"""
    try:
        # ã‚±ãƒ¼ã‚¹åˆ¥ã®æ§‹é€ åŒ–å‡¦ç†
        if "ãƒˆã‚¤ãƒ¬" in filename:
            return process_toilet_content(content)
        elif "ãƒãƒƒãƒ†ãƒªãƒ¼" in filename:
            return process_battery_content(content)
        elif "ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼" in filename:
            return process_subbattery_content(content)
        elif "ãƒ‰ã‚¢" in filename or "çª“" in filename:
            return process_door_window_content(content)
        else:
            return process_general_content(content)
    except Exception as e:
        print(f"Warning: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({filename}): {e}")
        return content

def process_toilet_content(content):
    """ãƒˆã‚¤ãƒ¬é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ åŒ–"""
    # å…ƒã®æ§‹é€ ã‚’ä¿æŒã—ã¤ã¤ã€ã‚±ãƒ¼ã‚¹æƒ…å ±ã‚‚æŠ½å‡º
    import re
    
    # ã‚±ãƒ¼ã‚¹ã‚’åˆ†å‰²
    cases = re.split(r'## \[Case ([^\]]+)\]', content)
    structured_content = []
    
    if len(cases) > 1:  # ã‚±ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
        for i in range(1, len(cases), 2):
            if i + 1 < len(cases):
                case_id = cases[i]
                case_content = cases[i + 1]
                
                # ç—‡çŠ¶ã¨è§£æ±ºç­–ã‚’æŠ½å‡º
                symptoms = extract_symptoms(case_content)
                solutions = extract_solutions(case_content)
                costs = extract_costs(case_content)
                
                structured_case = f"""
ã‚±ãƒ¼ã‚¹ID: {case_id}
ç—‡çŠ¶: {symptoms}
è§£æ±ºç­–: {solutions}
è²»ç”¨ç›®å®‰: {costs}
è©³ç´°: {case_content}
"""
                structured_content.append(structured_case)
        
        # ã‚±ãƒ¼ã‚¹æƒ…å ±ã¨å…ƒã®å†…å®¹ã‚’çµ„ã¿åˆã‚ã›
        return "\n".join(structured_content) + "\n\n---\n\n" + content
    else:
        # ã‚±ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…ƒã®å†…å®¹ã‚’ãã®ã¾ã¾è¿”ã™
        return content

def process_battery_content(content):
    """ãƒãƒƒãƒ†ãƒªãƒ¼é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ åŒ–"""
    # å…ƒã®æ§‹é€ ã‚’ä¿æŒã—ã¤ã¤ã€ã‚±ãƒ¼ã‚¹æƒ…å ±ã‚‚æŠ½å‡º
    import re
    
    # ã‚±ãƒ¼ã‚¹ã‚’åˆ†å‰²
    cases = re.split(r'## \[Case ([^\]]+)\]', content)
    structured_content = []
    
    if len(cases) > 1:  # ã‚±ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
        for i in range(1, len(cases), 2):
            if i + 1 < len(cases):
                case_id = cases[i]
                case_content = cases[i + 1]
                
                # ãƒãƒƒãƒ†ãƒªãƒ¼ç‰¹æœ‰ã®æƒ…å ±ã‚’æŠ½å‡º
                voltage_info = extract_voltage_info(case_content)
                charging_info = extract_charging_info(case_content)
                maintenance_info = extract_maintenance_info(case_content)
                
                structured_case = f"""
ã‚±ãƒ¼ã‚¹ID: {case_id}
é›»åœ§æƒ…å ±: {voltage_info}
å……é›»æƒ…å ±: {charging_info}
ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æƒ…å ±: {maintenance_info}
è©³ç´°: {case_content}
"""
                structured_content.append(structured_case)
        
        # ã‚±ãƒ¼ã‚¹æƒ…å ±ã¨å…ƒã®å†…å®¹ã‚’çµ„ã¿åˆã‚ã›
        return "\n".join(structured_content) + "\n\n---\n\n" + content
    else:
        # ã‚±ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…ƒã®å†…å®¹ã‚’ãã®ã¾ã¾è¿”ã™
        return content

def process_subbattery_content(content):
    """ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ åŒ–"""
    import re
    
    # ã‚±ãƒ¼ã‚¹ã‚’åˆ†å‰²
    cases = re.split(r'## \[Case ([^\]]+)\]', content)
    structured_content = []
    
    for i in range(1, len(cases), 2):
        if i + 1 < len(cases):
            case_id = cases[i]
            case_content = cases[i + 1]
            
            # ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼ç‰¹æœ‰ã®æƒ…å ±ã‚’æŠ½å‡º
            capacity_info = extract_capacity_info(case_content)
            charging_system = extract_charging_system(case_content)
            usage_pattern = extract_usage_pattern(case_content)
            
            structured_case = f"""
ã‚±ãƒ¼ã‚¹ID: {case_id}
å®¹é‡æƒ…å ±: {capacity_info}
å……é›»ã‚·ã‚¹ãƒ†ãƒ : {charging_system}
ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³: {usage_pattern}
è©³ç´°: {case_content}
"""
            structured_content.append(structured_case)
    
    return "\n".join(structured_content)


def process_door_window_content(content):
    """ãƒ‰ã‚¢ãƒ»çª“é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ åŒ–"""
    import re
    
    # DW-ã‚±ãƒ¼ã‚¹ã‚’æŠ½å‡º
    cases = re.findall(r'### DW-(\d+): ([^\n]+)\n\*\*ç—‡çŠ¶\*\*: ([^\n]+)\n\*\*åŸå› \*\*: ([^\n]+)\n\*\*å¯¾å‡¦æ³•\*\*: ([^\n]+)\n\*\*ä¿®ç†æ™‚é–“\*\*: ([^\n]+)\n\*\*è²»ç”¨ç›®å®‰\*\*: ([^\n]+)', content)
    
    structured_cases = []
    for case in cases:
        case_id, title, symptoms, cause, solution, time, cost = case
        structured_cases.append({
            'case_id': f'DW-{case_id}',
            'title': title,
            'symptoms': symptoms,
            'cause': cause,
            'solution': solution,
            'time': time,
            'cost': cost
        })
    
    # æ§‹é€ åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆ
    structured_content = f"# ãƒ‰ã‚¢ãƒ»çª“ã®é–‹é–‰ä¸è‰¯ - ä¿®ç†ã‚±ãƒ¼ã‚¹ä¸€è¦§\n\n"
    
    for case in structured_cases:
        structured_content += f"## {case['case_id']}: {case['title']}\n"
        structured_content += f"**ç—‡çŠ¶**: {case['symptoms']}\n"
        structured_content += f"**åŸå› **: {case['cause']}\n"
        structured_content += f"**å¯¾å‡¦æ³•**: {case['solution']}\n"
        structured_content += f"**ä¿®ç†æ™‚é–“**: {case['time']}\n"
        structured_content += f"**è²»ç”¨ç›®å®‰**: {case['cost']}\n\n"
    
    # å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚‚å«ã‚ã‚‹
    structured_content += "\n---\n\n" + content
    
    return structured_content


def process_general_content(content):
    """ä¸€èˆ¬çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ åŒ–"""
    # åŸºæœ¬çš„ãªæ§‹é€ åŒ–å‡¦ç†
    import re
    
    # è¦‹å‡ºã—ã‚’æŠ½å‡º
    headers = re.findall(r'^#+\s+(.+)$', content, re.MULTILINE)
    
    # ãƒªã‚¹ãƒˆé …ç›®ã‚’æŠ½å‡º
    list_items = re.findall(r'^[-*+]\s+(.+)$', content, re.MULTILINE)
    
    structured_content = f"""
è¦‹å‡ºã—: {' | '.join(headers)}
ãƒªã‚¹ãƒˆé …ç›®: {' | '.join(list_items)}
å†…å®¹: {content}
"""
    return structured_content


def extract_symptoms(content):
    """ç—‡çŠ¶ã‚’æŠ½å‡º"""
    import re
    symptoms = re.findall(r'ç—‡çŠ¶[ï¼š:]\s*(.+?)(?=\n|$)', content, re.DOTALL)
    return ' | '.join(symptoms) if symptoms else "ç—‡çŠ¶æƒ…å ±ãªã—"


def extract_solutions(content):
    """è§£æ±ºç­–ã‚’æŠ½å‡º"""
    import re
    solutions = re.findall(r'è§£æ±º[ç­–æ³•][ï¼š:]\s*(.+?)(?=\n|$)', content, re.DOTALL)
    return ' | '.join(solutions) if solutions else "è§£æ±ºç­–æƒ…å ±ãªã—"


def extract_costs(content):
    """è²»ç”¨æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    costs = re.findall(r'(\d+[,ï¼Œ]\d+å††|\d+å††)', content)
    return ' | '.join(costs) if costs else "è²»ç”¨æƒ…å ±ãªã—"


def extract_voltage_info(content):
    """é›»åœ§æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    voltages = re.findall(r'(\d+\.\d+V|\d+V)', content)
    return ' | '.join(voltages) if voltages else "é›»åœ§æƒ…å ±ãªã—"


def extract_charging_info(content):
    """å……é›»æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    charging = re.findall(r'(å……é›»|ãƒãƒ£ãƒ¼ã‚¸|ã‚¢ã‚¤ã‚½ãƒ¬ãƒ¼ã‚¿ãƒ¼|DC-DC)', content)
    return ' | '.join(charging) if charging else "å……é›»æƒ…å ±ãªã—"


def extract_maintenance_info(content):
    """ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    maintenance = re.findall(r'(ç‚¹æ¤œ|æ¸…æƒ|äº¤æ›|ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹)', content)
    return ' | '.join(maintenance) if maintenance else "ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æƒ…å ±ãªã—"


def extract_capacity_info(content):
    """å®¹é‡æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    capacity = re.findall(r'(\d+Ah|\d+W|\d+Wh)', content)
    return ' | '.join(capacity) if capacity else "å®¹é‡æƒ…å ±ãªã—"


def extract_charging_system(content):
    """å……é›»ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    system = re.findall(r'(ã‚¢ã‚¤ã‚½ãƒ¬ãƒ¼ã‚¿ãƒ¼|DC-DC|ãƒªãƒ¬ãƒ¼|ãƒ’ãƒ¥ãƒ¼ã‚º)', content)
    return ' | '.join(system) if system else "å……é›»ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ãªã—"


def extract_usage_pattern(content):
    """ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ã‚’æŠ½å‡º"""
    import re
    usage = re.findall(r'(å†·è”µåº«|ã‚¨ã‚¢ã‚³ãƒ³|ãƒ†ãƒ¬ãƒ“|ç…§æ˜|24æ™‚é–“)', content)
    return ' | '.join(usage) if usage else "ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ãªã—"


def create_enhanced_rag_system():
    """ãƒ–ãƒ­ã‚°URLã‚‚å«ã‚ãŸRAGã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ"""
    
    # æ—¢å­˜ã®Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å‰Šé™¤ï¼ˆå®‰å…¨ãªæ–¹æ³•ï¼‰
    chroma_db_path = "./chroma_db"
    if os.path.exists(chroma_db_path):
        print("æ—¢å­˜ã®Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å‰Šé™¤ä¸­...")
        try:
            # ãƒ—ãƒ­ã‚»ã‚¹ãŒä½¿ç”¨ä¸­ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            import time
            time.sleep(1)  # å°‘ã—å¾…æ©Ÿ
            shutil.rmtree(chroma_db_path)
            print("âœ… æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ’¡ æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¦ç¶šè¡Œã—ã¾ã™")
    
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
    embeddings_model = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
    
    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆã¯å†åˆ©ç”¨ã‚’è©¦è¡Œ
    if os.path.exists(chroma_db_path):
        try:
            print("ğŸ”„ æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            db = Chroma(persist_directory=chroma_db_path, embedding_function=embeddings_model)
            print("âœ… æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            return db
        except Exception as e:
            print(f"âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã™...")
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æº–å‚™
    documents = []

    # PDFãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
    main_path = os.path.dirname(os.path.abspath(__file__))
    pdf_path = os.path.join(main_path, "ã‚­ãƒ£ãƒ³ãƒ”ãƒ³ã‚°ã‚«ãƒ¼ä¿®ç†ãƒãƒ‹ãƒ¥ã‚¢ãƒ«.pdf")
    
    if os.path.exists(pdf_path):
        try:
            loader = PyPDFLoader(pdf_path)
            pdf_docs = loader.load()
            for doc in pdf_docs:
                if not isinstance(doc.page_content, str):
                    doc.page_content = str(doc.page_content)
                doc.metadata["source_type"] = "manual"
                doc.metadata["url"] = "ã‚­ãƒ£ãƒ³ãƒ”ãƒ³ã‚°ã‚«ãƒ¼ä¿®ç†ãƒãƒ‹ãƒ¥ã‚¢ãƒ«.pdf"
                documents.append(doc)
            print(f"âœ… PDFãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {len(pdf_docs)} ä»¶ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except Exception as e:
            print(f"âš ï¸ PDFèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    txt_files = glob.glob(os.path.join(main_path, "*.txt"))
    for txt_file in txt_files:
        try:
            loader = TextLoader(txt_file, encoding='utf-8')
            txt_docs = loader.load()
            for doc in txt_docs:
                if not isinstance(doc.page_content, str):
                    doc.page_content = str(doc.page_content)
                
                # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ åŒ–å‡¦ç†
                processed_content = process_markdown_content(doc.page_content, os.path.basename(txt_file))
                
                doc.page_content = processed_content
                doc.metadata["source_type"] = "text_file"
                doc.metadata["url"] = os.path.basename(txt_file)
                doc.metadata["title"] = os.path.basename(txt_file).replace('.txt', '')
                doc.metadata["content_type"] = "markdown_structured"
                documents.append(doc)
            print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(txt_file)} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except Exception as e:
            print(f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {txt_file} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ–ãƒ­ã‚°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
    blog_documents = [
        {
            "title": "ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼å®Œå…¨ã‚¬ã‚¤ãƒ‰",
            "content": "ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼ã€ãƒ‡ã‚£ãƒ¼ãƒ—ã‚µã‚¤ã‚¯ãƒ«ã€ãƒªãƒã‚¦ãƒ ã‚¤ã‚ªãƒ³ã€é‰›ãƒãƒƒãƒ†ãƒªãƒ¼ã€å®¹é‡é¸å®šã€å¯¿å‘½ã€å……é›»æ–¹æ³•ã€ãƒãƒƒãƒ†ãƒªãƒ¼ç®¡ç†ã€æ”¾é›»æ·±åº¦ã€æ®‹é‡è¨ˆã€é‹ç”¨æ™‚é–“ã€ãƒãƒƒãƒ†ãƒªãƒ¼ä¸¦åˆ—ã€ç›´åˆ—æ¥ç¶šã€æ¸©åº¦ç®¡ç†ã€ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã€å–ã‚Šä»˜ã‘æ–¹æ³•ã€é›»åœ§ç›£è¦–ã€è»Šä¸¡æ”¹é€ ã€ä¿å®‰åŸºæº–ã€äº¤æ›ç›®å®‰",
            "url": "https://camper-repair.net/blog/risk1/",
            "tags": ["ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼", "å®Œå…¨ã‚¬ã‚¤ãƒ‰", "å®¹é‡é¸å®š", "å¯¿å‘½"]
        },
        {
            "title": "ãƒãƒƒãƒ†ãƒªãƒ¼ãƒ»ãƒãƒƒãƒ†ãƒªãƒ¼ã®æ•…éšœã¨ä¿®ç†æ–¹æ³•",
            "content": "ãƒãƒƒãƒ†ãƒªãƒ¼æ•…éšœã€å……é›»ä¸è‰¯ã€é›»åœ§ä½ä¸‹ã€å§‹å‹•ä¸è‰¯ã€ãƒãƒƒãƒ†ãƒªãƒ¼äº¤æ›ã€å……é›»å™¨æ•…éšœã€ç«¯å­è…é£Ÿã€é›»è§£æ¶²ä¸è¶³ã€ãƒãƒƒãƒ†ãƒªãƒ¼è¨ºæ–­ã€é›»åœ§æ¸¬å®šã€å……é›»ã‚·ã‚¹ãƒ†ãƒ ã€ã‚¢ã‚¤ã‚½ãƒ¬ãƒ¼ã‚¿ãƒ¼ã€DC-DCã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ã€ãƒãƒƒãƒ†ãƒªãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã€BMSã€ãƒªãƒã‚¦ãƒ ã‚¤ã‚ªãƒ³ãƒãƒƒãƒ†ãƒªãƒ¼ã€é‰›ãƒãƒƒãƒ†ãƒªãƒ¼ã€AGMãƒãƒƒãƒ†ãƒªãƒ¼ã€ãƒ‡ã‚£ãƒ¼ãƒ—ã‚µã‚¤ã‚¯ãƒ«ãƒãƒƒãƒ†ãƒªãƒ¼",
            "url": "https://camper-repair.net/blog/repair1/",
            "tags": ["ãƒãƒƒãƒ†ãƒªãƒ¼", "æ•…éšœ", "ä¿®ç†æ–¹æ³•", "å……é›»ã‚·ã‚¹ãƒ†ãƒ "]
        },
        {
            "title": "å®šæœŸç‚¹æ¤œã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹",
            "content": "å®šæœŸç‚¹æ¤œã€ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã€äºˆé˜²ä¿å…¨ã€ç‚¹æ¤œé …ç›®ã€ãƒãƒƒãƒ†ãƒªãƒ¼ç‚¹æ¤œã€é›»è£…ç³»ç‚¹æ¤œã€æ°´å›ã‚Šç‚¹æ¤œã€ã‚¬ã‚¹ç³»ç‚¹æ¤œã€ã‚¨ã‚¢ã‚³ãƒ³ç‚¹æ¤œã€å†·è”µåº«ç‚¹æ¤œã€ãƒˆã‚¤ãƒ¬ç‚¹æ¤œã€çµ¦æ°´ã‚·ã‚¹ãƒ†ãƒ ã€æ’æ°´ã‚·ã‚¹ãƒ†ãƒ ã€ã‚¬ã‚¹é…ç®¡ã€é›»æ°—é…ç·šã€å®‰å…¨è£…ç½®ã€æ¶ˆé˜²è¨­å‚™ã€ç‚¹æ¤œè¨˜éŒ²ã€ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«",
            "url": "https://camper-repair.net/battery-selection/",
            "tags": ["å®šæœŸç‚¹æ¤œ", "ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹", "äºˆé˜²ä¿å…¨", "ç‚¹æ¤œé …ç›®"]
        }
    ]
    
    for blog in blog_documents:
        doc = Document(
            page_content=f"ã‚¿ã‚¤ãƒˆãƒ«: {blog['title']}\n\nå†…å®¹: {blog['content']}\n\nã‚¿ã‚°: {', '.join(blog['tags'])}",
            metadata={
                "title": blog['title'],
                "url": blog['url'],
                "tags": ', '.join(blog['tags']),
                "source_type": "blog"
            }
        )
        documents.append(doc)
    
    print(f"âœ… ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)} ä»¶")
    
    # ChromaDBãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    if not CHROMA_AVAILABLE:
        raise ImportError("ChromaDBãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚langchain-chromaã¨chromadbã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
    try:
        print("æ–°ã—ã„Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆä¸­...")
        db = Chroma.from_documents(
            documents=documents, 
            embedding=embeddings_model,
            persist_directory=chroma_db_path
        )
        print("âœ… Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸ")
        return db
    except Exception as e:
        print(f"âŒ Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def enhanced_rag_retrieve(question: str, db, max_results: int = 5):
    """æ‹¡å¼µRAGæ¤œç´¢ï¼ˆãƒ–ãƒ­ã‚°URLã‚‚å«ã‚€ï¼‰"""
    if db is None:
        return {
            "manual_content": "ChromaDBãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
            "text_file_content": "ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
            "blog_links": []
        }
    
    try:
        # similarity_searchã‚’ç›´æ¥ä½¿ç”¨
        docs = db.similarity_search(question, k=max_results)
        
        # çµæœã‚’æ•´ç†
        manual_content = []
        text_file_content = []
        blog_links = []
        
        for doc in docs:
            if doc.metadata.get("source_type") == "manual":
                manual_content.append(doc.page_content)
            elif doc.metadata.get("source_type") == "text_file":
                text_file_content.append(doc.page_content)
            elif doc.metadata.get("source_type") == "blog":
                tags_str = doc.metadata.get("tags", "")
                tags = [tag.strip() for tag in tags_str.split(',')] if tags_str else []
                
                blog_links.append({
                    "title": doc.metadata.get("title", "ãƒ–ãƒ­ã‚°è¨˜äº‹"),
                    "url": doc.metadata.get("url", "#"),
                    "content": doc.page_content,
                    "tags": tags
                })
        
        return {
            "manual_content": "\n".join(manual_content),
            "text_file_content": "\n".join(text_file_content),
            "blog_links": blog_links
        }
    except Exception as e:
        print(f"âŒ RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            "manual_content": "æ¤œç´¢ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
            "text_file_content": "æ¤œç´¢ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
            "blog_links": []
        }


def format_blog_links(blog_links, max_links: int = 3):
    """ãƒ–ãƒ­ã‚°ãƒªãƒ³ã‚¯ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not blog_links:
        return ""
    
    formatted_links = []
    for i, blog in enumerate(blog_links[:max_links]):
        formatted_links.append(f"â€¢ {blog['title']}: {blog['url']}")
    
    return "\n".join(formatted_links)


# ä½¿ç”¨ä¾‹
def create_notion_based_rag_system(use_text_files=False):
    """
    Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ã®æ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ
    
    Args:
        use_text_files (bool): ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å«ã‚ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Falseï¼‰
    
    Returns:
        Chroma: ChromaDBã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯None
    """
    
    # æ—¢å­˜ã®Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹
    chroma_db_path = "./chroma_db"
    
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
    embeddings_model = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æº–å‚™
    documents = []
    
    # Notionã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’å–å¾—
    print("ğŸ”„ Notionã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    try:
        from data_access.notion_client import notion_client
        print(f"ğŸ” Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆçŠ¶æ…‹: {notion_client is not None}")
        if notion_client:
            print("ğŸ” Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–çŠ¶æ³ã‚’ç¢ºèªä¸­...")
            knowledge_items = notion_client.load_knowledge_base()
            print(f"ğŸ“Š å–å¾—ã—ãŸãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ æ•°: {len(knowledge_items) if knowledge_items else 0}")
        else:
            print("âŒ Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            knowledge_items = None
        
        if knowledge_items:
            for item in knowledge_items:
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’æ§‹ç¯‰
                content_parts = []
                
                # ã™ã¹ã¦ã®å€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’é˜²æ­¢
                if item.get("title"):
                    content_parts.append(f"ã‚¿ã‚¤ãƒˆãƒ«: {str(item['title'])}")
                
                if item.get("category"):
                    content_parts.append(f"ã‚«ãƒ†ã‚´ãƒª: {str(item['category'])}")
                
                if item.get("content"):
                    content_parts.append(f"\nå†…å®¹:\n{str(item['content'])}")
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡¦ç†ï¼ˆãƒªã‚¹ãƒˆã¾ãŸã¯æ–‡å­—åˆ—ã«å¯¾å¿œï¼‰
                keywords = item.get("keywords")
                if keywords:
                    if isinstance(keywords, list):
                        keywords_str = ', '.join(str(k) for k in keywords if k)
                    else:
                        keywords_str = str(keywords)
                    if keywords_str:
                        content_parts.append(f"\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords_str}")
                
                # ã‚¿ã‚°ã®å‡¦ç†ï¼ˆãƒªã‚¹ãƒˆã¾ãŸã¯æ–‡å­—åˆ—ã«å¯¾å¿œï¼‰
                tags = item.get("tags")
                if tags:
                    if isinstance(tags, list):
                        tags_str = ', '.join(str(t) for t in tags if t)
                    else:
                        tags_str = str(tags)
                    if tags_str:
                        content_parts.append(f"\nã‚¿ã‚°: {tags_str}")
                
                # æœ€ä½é™ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                if not content_parts:
                    content_parts.append("ãƒ‡ãƒ¼ã‚¿ãªã—")
                
                # Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                doc = Document(
                    page_content="\n".join(content_parts),
                    metadata={
                        "title": str(item.get("title", "")),
                        "category": str(item.get("category", "")),
                        "url": str(item.get("url", "")),
                        "source_type": "notion_knowledge_base",
                        "notion_id": str(item.get("id", ""))
                    }
                )
                documents.append(doc)
            
            print(f"âœ… NotionãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹: {len(knowledge_items)}ä»¶ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        else:
            print("âš ï¸ NotionãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãŒç©ºã§ã™")
    
    except ImportError as e:
        print(f"âš ï¸ Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    except Exception as e:
        print(f"âš ï¸ Notionãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å«ã‚ã‚‹
    if use_text_files:
        print("ğŸ”„ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚‚èª­ã¿è¾¼ã¿ä¸­...")
        main_path = os.path.dirname(os.path.abspath(__file__))
        txt_files = glob.glob(os.path.join(main_path, "*.txt"))
        
        for txt_file in txt_files:
            try:
                loader = TextLoader(txt_file, encoding='utf-8')
                txt_docs = loader.load()
                
                for doc in txt_docs:
                    if not isinstance(doc.page_content, str):
                        doc.page_content = str(doc.page_content)
                    
                    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ åŒ–å‡¦ç†
                    processed_content = process_markdown_content(doc.page_content, os.path.basename(txt_file))
                    
                    doc.page_content = processed_content
                    doc.metadata["source_type"] = "text_file"
                    doc.metadata["url"] = os.path.basename(txt_file)
                    doc.metadata["title"] = os.path.basename(txt_file).replace('.txt', '')
                    doc.metadata["content_type"] = "markdown_structured"
                    documents.append(doc)
                
                print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(txt_file)} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            except Exception as e:
                print(f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« {txt_file} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ä¿®ç†ã‚±ãƒ¼ã‚¹ã‚‚Notionã‹ã‚‰å–å¾—ã—ã¦è¿½åŠ 
    print("ğŸ”„ Notionã‹ã‚‰ä¿®ç†ã‚±ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    try:
        from data_access.notion_client import notion_client
        repair_cases = notion_client.load_repair_cases()
        
        if repair_cases:
            for case in repair_cases:
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’æ§‹ç¯‰
                content_parts = []
                
                # ã™ã¹ã¦ã®å€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’é˜²æ­¢
                if case.get("title"):
                    content_parts.append(f"ã‚±ãƒ¼ã‚¹ID: {str(case['title'])}")
                
                if case.get("category"):
                    content_parts.append(f"ã‚«ãƒ†ã‚´ãƒª: {str(case['category'])}")
                
                # ç—‡çŠ¶ã®å‡¦ç†ï¼ˆãƒªã‚¹ãƒˆã¾ãŸã¯æ–‡å­—åˆ—ã«å¯¾å¿œï¼‰
                symptoms = case.get("symptoms")
                if symptoms:
                    if isinstance(symptoms, list):
                        symptoms_str = ', '.join(str(s) for s in symptoms if s)
                    else:
                        symptoms_str = str(symptoms)
                    if symptoms_str:
                        content_parts.append(f"ç—‡çŠ¶: {symptoms_str}")
                
                if case.get("solution"):
                    content_parts.append(f"è§£æ±ºæ–¹æ³•: {str(case['solution'])}")
                
                if case.get("cost_estimate"):
                    content_parts.append(f"è²»ç”¨è¦‹ç©ã‚‚ã‚Š: {str(case['cost_estimate'])}")
                
                if case.get("difficulty"):
                    content_parts.append(f"é›£æ˜“åº¦: {str(case['difficulty'])}")
                
                # æœ€ä½é™ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                if not content_parts:
                    content_parts.append("ã‚±ãƒ¼ã‚¹æƒ…å ±")
                
                # Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                doc = Document(
                    page_content="\n".join(content_parts),
                    metadata={
                        "title": str(case.get("title", "")),
                        "category": str(case.get("category", "")),
                        "source_type": "notion_repair_case",
                        "notion_id": str(case.get("id", ""))
                    }
                )
                documents.append(doc)
            
            print(f"âœ… Notionä¿®ç†ã‚±ãƒ¼ã‚¹: {len(repair_cases)}ä»¶ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        else:
            print("âš ï¸ Notionä¿®ç†ã‚±ãƒ¼ã‚¹ãŒç©ºã§ã™")
    
    except Exception as e:
        print(f"âš ï¸ Notionä¿®ç†ã‚±ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"âœ… ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}ä»¶")
    
    if len(documents) == 0:
        print("âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒ1ä»¶ã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚RAGã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆã§ãã¾ã›ã‚“ã€‚")
        return None
    
    # ChromaDBãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    if not CHROMA_AVAILABLE:
        print("âŒ ChromaDBãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚langchain-chromaã¨chromadbã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        return None
    
    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¦æ–°è¦ä½œæˆ
    if os.path.exists(chroma_db_path):
        print("ğŸ”„ æ—¢å­˜ã®Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å‰Šé™¤ä¸­...")
        try:
            import time
            time.sleep(1)
            shutil.rmtree(chroma_db_path)
            print("âœ… æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
    try:
        print("ğŸ”„ æ–°ã—ã„Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆä¸­...")
        print(f"ğŸ“Š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¤œè¨¼ã¨ä¿®æ­£
        print("ğŸ” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¤œè¨¼ä¸­...")
        valid_documents = []
        
        for i, doc in enumerate(documents):
            try:
                # page_contentã®æ¤œè¨¼
                if not isinstance(doc.page_content, str):
                    print(f"âš ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{i}: page_contentãŒæ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆå‹: {type(doc.page_content)}ï¼‰")
                    doc.page_content = str(doc.page_content)
                
                # ç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒã‚§ãƒƒã‚¯
                if not doc.page_content or len(doc.page_content) < 1:
                    print(f"âš ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{i}: page_contentãŒç©ºã§ã™ - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                    continue
                
                # metadataã®æ¤œè¨¼ã¨ä¿®æ­£
                if doc.metadata:
                    cleaned_metadata = {}
                    for key, value in doc.metadata.items():
                        if value is None:
                            cleaned_metadata[key] = ""
                        elif isinstance(value, (str, int, float, bool)):
                            cleaned_metadata[key] = str(value) if not isinstance(value, (int, float, bool)) else value
                        elif isinstance(value, list):
                            # ãƒªã‚¹ãƒˆã¯æ–‡å­—åˆ—ã«å¤‰æ›
                            cleaned_metadata[key] = ', '.join(str(v) for v in value)
                        else:
                            print(f"âš ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{i}: metadata['{key}']ã®å‹ãŒä¸æ­£ï¼ˆå‹: {type(value)}ï¼‰ - æ–‡å­—åˆ—ã«å¤‰æ›")
                            cleaned_metadata[key] = str(value)
                    doc.metadata = cleaned_metadata
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œè¨¼æ¸ˆã¿ãƒªã‚¹ãƒˆã«è¿½åŠ 
                valid_documents.append(doc)
                    
            except Exception as e:
                print(f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{i}ã®æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e} - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                continue
        
        print(f"âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œè¨¼å®Œäº†: {len(valid_documents)}/{len(documents)}ä»¶ãŒæœ‰åŠ¹")
        
        if len(valid_documents) == 0:
            print("âŒ æœ‰åŠ¹ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒ1ä»¶ã‚‚ã‚ã‚Šã¾ã›ã‚“")
            return None
        
        # ChromaDBã‚’ä½œæˆï¼ˆãƒãƒƒãƒå‡¦ç†ã§å•é¡Œã®ã‚ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç‰¹å®šï¼‰
        print(f"ğŸ”„ ChromaDBã«{len(valid_documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ä¸­...")
        
        # å°ãƒãƒƒãƒã§ãƒ†ã‚¹ãƒˆã—ã¦å•é¡Œã®ã‚ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç‰¹å®š
        batch_size = 10
        final_valid_documents = []
        
        for batch_start in range(0, len(valid_documents), batch_size):
            batch_end = min(batch_start + batch_size, len(valid_documents))
            batch = valid_documents[batch_start:batch_end]
            
            try:
                # ã“ã®ãƒãƒƒãƒã‚’ãƒ†ã‚¹ãƒˆ
                print(f"  ğŸ”„ ãƒãƒƒãƒ {batch_start//batch_size + 1}: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{batch_start}-{batch_end-1} ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
                
                # å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å€‹åˆ¥ã«ãƒã‚§ãƒƒã‚¯
                for j, doc in enumerate(batch):
                    doc_index = batch_start + j
                    try:
                        # page_contentã¨metadataã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
                        if not isinstance(doc.page_content, str):
                            print(f"    âš ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{doc_index}: page_contentå‹ã‚¨ãƒ©ãƒ¼ - ä¿®æ­£")
                            doc.page_content = str(doc.page_content)
                        
                        # metadataã®å…¨ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
                        if doc.metadata:
                            for k, v in list(doc.metadata.items()):
                                if isinstance(v, list):
                                    print(f"    âš ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{doc_index}: metadata['{k}']ãŒãƒªã‚¹ãƒˆ - æ–‡å­—åˆ—ã«å¤‰æ›")
                                    doc.metadata[k] = ', '.join(str(item) for item in v)
                                elif not isinstance(v, (str, int, float, bool, type(None))):
                                    print(f"    âš ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{doc_index}: metadata['{k}']ã®å‹ãŒä¸æ­£ï¼ˆ{type(v)}ï¼‰- æ–‡å­—åˆ—ã«å¤‰æ›")
                                    doc.metadata[k] = str(v)
                        
                        final_valid_documents.append(doc)
                        
                    except Exception as e:
                        print(f"    âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{doc_index}ã§ã‚¨ãƒ©ãƒ¼: {e} - ã‚¹ã‚­ãƒƒãƒ—")
                        print(f"       page_content: {str(doc.page_content)[:50]}...")
                        print(f"       metadata: {doc.metadata}")
                        continue
                
                print(f"  âœ… ãƒãƒƒãƒ{batch_start//batch_size + 1}å®Œäº†: {len(batch)}ä»¶ä¸­{len([d for d in batch if d in final_valid_documents])}ä»¶æœ‰åŠ¹")
                
            except Exception as e:
                print(f"  âŒ ãƒãƒƒãƒ{batch_start//batch_size + 1}ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"âœ… æœ€çµ‚æ¤œè¨¼å®Œäº†: {len(final_valid_documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒä½¿ç”¨å¯èƒ½")
        
        if len(final_valid_documents) == 0:
            print("âŒ ä½¿ç”¨å¯èƒ½ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒ1ä»¶ã‚‚ã‚ã‚Šã¾ã›ã‚“")
            return None
        
        # ChromaDBã‚’ä½œæˆ
        print(f"ğŸ”„ ChromaDBã«{len(final_valid_documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åŸ‹ã‚è¾¼ã¿é–‹å§‹...")
        db = Chroma.from_documents(
            documents=final_valid_documents,
            embedding=embeddings_model,
            persist_directory=chroma_db_path
        )
        print("âœ… Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸ")
        return db
        
    except Exception as e:
        print(f"âŒ Chromaãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼å‹: {type(e)}")
        import traceback
        print("âŒ ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:")
        traceback.print_exc()
        
        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¡¨ç¤º
        print("\nğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
        print(f"  - å…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç·æ•°: {len(documents)}")
        print(f"  - æœ‰åŠ¹ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(valid_documents) if 'valid_documents' in locals() else 'N/A'}")
        
        if len(documents) > 0:
            print(f"\n  - ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:")
            for i in range(min(3, len(documents))):
                print(f"    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ{i}:")
                print(f"      - page_contentå‹: {type(documents[i].page_content)}")
                print(f"      - page_contenté•·: {len(documents[i].page_content) if isinstance(documents[i].page_content, str) else 'N/A'}")
                print(f"      - page_content: {str(documents[i].page_content)[:100]}...")
                print(f"      - metadata: {documents[i].metadata}")
        
        return None


if __name__ == "__main__":
    print("=== Notionçµ±åˆRAGã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ ===")
    
    # Notionçµ±åˆRAGã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ
    db = create_notion_based_rag_system(use_text_files=False)
    
    if db:
        # ãƒ†ã‚¹ãƒˆæ¤œç´¢
        question = "ã‚µãƒ–ãƒãƒƒãƒ†ãƒªãƒ¼ã®èª¿å­ãŒæ‚ªã„"
        results = enhanced_rag_retrieve(question, db)
        
        print("\n=== æ¤œç´¢çµæœ ===")
        print(f"è³ªå•: {question}")
        print(f"\nãƒãƒ‹ãƒ¥ã‚¢ãƒ«å†…å®¹: {results['manual_content'][:200] if results['manual_content'] else 'ãªã—'}...")
        print(f"\nãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹: {results['text_file_content'][:200] if results['text_file_content'] else 'ãªã—'}...")
        print(f"\né–¢é€£ãƒ–ãƒ­ã‚°:")
        for blog in results.get('blog_links', []):
            print(f"â€¢ {blog['title']}: {blog['url']}")
    else:
        print("âŒ RAGã‚·ã‚¹ãƒ†ãƒ ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")